{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0914f1f6",
   "metadata": {},
   "source": [
    "# Modelling:\n",
    "- This notebook uses the data obtained from Pre-Processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2674903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "tf.keras.utils.set_random_seed(442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 14:07:06.968913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 14:07:07.010707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 14:07:07.010862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# GPU Usage\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Set memory growth\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'Sadness': 0,\n",
    "    'Excited': 1,\n",
    "    'Happiness': 2,\n",
    "    'Anger' : 3,\n",
    "    'Frustration' : 4,\n",
    "    'Other' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf9f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    one_hot = np.zeros(6)\n",
    "    one_hot[labeldict[label]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(listOfLabels):\n",
    "    finalList = []\n",
    "    for label in listOfLabels:\n",
    "        finalList.append(one_hot_encode(label))\n",
    "    return np.array(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_and_label(path):\n",
    "    emotion = re.match('.*/DATA/([a-zA-Z]+)/.*', path).groups()[0]\n",
    "    data, _ = librosa.load(path, sr=44100)\n",
    "    STFT = np.abs(librosa.stft(data))\n",
    "    return STFT, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(pathList): # Returns a list of x (batch_size, timesteps, feature), y (one_hot_encoded)\n",
    "    with mp.Pool() as p:\n",
    "        results = p.map(get_STFT_and_label, pathList)\n",
    "    # Preprocess x:\n",
    "    x = [item[0] for item in results]\n",
    "    # Flatten\n",
    "    x = [item for sublist in x for item in sublist]\n",
    "    # Zero-padding:\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", maxlen=1497, dtype = np.float32) # maxlen is after discovering the whole training data\n",
    "    # Reshaping so that the order is not messed up\n",
    "    x = x.reshape(-1, 1025, 1497)\n",
    "    # Transposing so that we have timesteps in dim 1\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    # Preprocess y:\n",
    "    y = [item[1] for item in results]\n",
    "    # one_hot_encode\n",
    "    y = one_hot_encode_list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5000",
   "metadata": {},
   "source": [
    "# Loading data: \n",
    "- We will load the data per predefined batch size, this is to reduce the memory used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fbf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_paths.pkl', 'rb') as f:\n",
    "    train_paths = pickle.load(f)\n",
    "with open('test_paths.pkl', 'rb') as f:\n",
    "    test_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc01af",
   "metadata": {},
   "source": [
    "### Verify the order is preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78ac068",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b=preprocess_input(train_paths[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69b089b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.8433094e-02, 1.3021269e-01, 1.4366210e-01, ..., 7.1329065e-04,\n",
       "        7.1328791e-04, 7.1328570e-04],\n",
       "       [2.4098808e-01, 3.4502932e-01, 3.7759700e-01, ..., 3.5598411e-04,\n",
       "        3.5598440e-04, 3.5597730e-04],\n",
       "       [1.0744733e-01, 4.3875551e-01, 4.3909615e-01, ..., 1.9026218e-10,\n",
       "        5.0048090e-09, 3.9142600e-09],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a12c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap, _ = get_STFT_and_label(train_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dddd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84330943e-02, 1.30212694e-01, 1.43662095e-01, ...,\n",
       "        7.13290647e-04, 7.13287911e-04, 7.13285699e-04],\n",
       "       [2.40988076e-01, 3.45029324e-01, 3.77597004e-01, ...,\n",
       "        3.55984113e-04, 3.55984404e-04, 3.55977303e-04],\n",
       "       [1.07447334e-01, 4.38755512e-01, 4.39096153e-01, ...,\n",
       "        1.90262181e-10, 5.00480901e-09, 3.91426003e-09],\n",
       "       ...,\n",
       "       [1.86033726e-01, 3.90104949e-01, 4.49180543e-01, ...,\n",
       "        1.15817738e-08, 7.79957610e-09, 2.30712938e-09],\n",
       "       [6.79136366e-02, 1.73671320e-01, 1.22675106e-01, ...,\n",
       "        4.54663714e-05, 4.54557012e-05, 4.54601577e-05],\n",
       "       [9.56224501e-02, 1.32804841e-01, 1.33970708e-01, ...,\n",
       "        1.33038717e-04, 1.33034511e-04, 1.33032721e-04]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "841dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batches of the pathList:\n",
    "def create_batches(pathList, batch_size):\n",
    "    ansList = [] # To store the final batched paths\n",
    "    tempList = [] # Temporary list\n",
    "    count = 0\n",
    "    while count < len(pathList):\n",
    "        tempList.append(pathList[count]) # Append the path\n",
    "        count += 1\n",
    "        if (count % batch_size) == 0: # if count is a multiple of batch_size\n",
    "            ansList.append(tempList)\n",
    "            tempList = []\n",
    "    if len(tempList) != 0: # If tempList is not empty\n",
    "        ansList.append(tempList) # Append the remaining values\n",
    "    return ansList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7d8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation datasets. The validation datasets are loaded entirely to the machine.\n",
    "x_val, y_val = preprocess_input(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f75e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1489, 1025)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28921b58",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras API:\n",
    "# inp = layers.Input(shape=(None, 1025))\n",
    "# x = layers.Masking(mask_value=0.0)(inp)\n",
    "# total_seq, final_hidden_state, final_cell_state = layers.LSTM(256, return_state=True)(x)\n",
    "# x = layers.Dense(512, activation='relu')(final_hidden_state)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8baf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras API: LSTM\n",
    "# inp = layers.Input(shape=(None, 1025))\n",
    "# x = layers.Masking(mask_value=0.0)(inp)\n",
    "# lstm1, final_hidden_state1, final_cell_state1 = layers.LSTM(256, return_state=True, return_sequences=True)(x)\n",
    "# lstm2, final_hidden_state2, final_cell_state2 = layers.LSTM(256, return_state=True, return_sequences=True)(lstm1, initial_state=[final_hidden_state1, final_cell_state1])\n",
    "# lstm3, final_hidden_state3, final_cell_state3 = layers.LSTM(256, return_state=True)(lstm2, initial_state=[final_hidden_state2, final_cell_state2])\n",
    "# concatenated = layers.Concatenate()([final_hidden_state3, final_cell_state3]) # concatenate the hidden state and cell state to make use of the information\n",
    "# x = layers.Flatten()(concatenated) # Flattening the hidden state\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5661fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras API: RNN + CNN\n",
    "inp = layers.Input(shape=(1489, 1025))\n",
    "\n",
    "conv1 = layers.Conv1D(64, 3)(inp)\n",
    "batchnorm1 = layers.BatchNormalization()(conv1)\n",
    "maxpool1 = layers.MaxPool1D(3)(batchnorm1)\n",
    "\n",
    "conv2 = layers.Conv1D(64, 3)(maxpool1)\n",
    "batchnorm2 = layers.BatchNormalization()(conv2)\n",
    "maxpool2 = layers.MaxPool1D(3)(batchnorm2)\n",
    "\n",
    "conv3 = layers.Conv1D(64, 3)(maxpool2)\n",
    "batchnorm3 = layers.BatchNormalization()(conv3)\n",
    "maxpool3 = layers.MaxPool1D(3)(batchnorm3)\n",
    "\n",
    "flattened = layers.Flatten()(maxpool3)\n",
    "\n",
    "x = layers.Dense(256, activation='relu')(flattened)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3908fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1489, 1025)]      0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 1487, 64)          196864    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 1487, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 495, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 493, 64)           12352     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 493, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 164, 64)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 162, 64)           12352     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 162, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPooling  (None, 54, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3456)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,148,870\n",
      "Trainable params: 1,148,486\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b122fa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28347024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size is 32, epochs = 30\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer is Stochastic Gradient Descent\n",
    "# Loss function is Categorical Crossentropy\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc25bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = create_batches(train_paths, batch_size=batch_size)\n",
    "validation_batch = create_batches(test_paths, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3369e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.CategoricalCrossentropy()\n",
    "validation_loss = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "538ba3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store epoch results:\n",
    "epoch_accuracy_train = []\n",
    "epoch_accuracy_val = []\n",
    "epoch_loss_train = []\n",
    "epoch_loss_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f3c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up, use graph execution\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training = True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # Update training accuracy\n",
    "    train_metrics.update_state(y, y_pred)\n",
    "    # Update training loss:\n",
    "    train_loss.update_state(y, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a850e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(x, y):\n",
    "    y_val_pred = model(x, training=False)\n",
    "    # Update metrics for validation\n",
    "    validation_metrics.update_state(y, y_val_pred)\n",
    "    validation_loss.update_state(y, y_val_pred)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1971ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 14:10:12.190435: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8400\n",
      "2022-05-22 14:10:13.783421: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 1.9474\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7161\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7999\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7671\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6444\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5879\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7164\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7452\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.8175\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6476\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5553\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.8433\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7205\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7312\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6243\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2717\n",
      "Training loss over epoch: 1.7066\n",
      "Validation acc: 0.3133\n",
      "Validation loss: 1.6695\n",
      "Time taken: 241.25s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.6354\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.5334\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6209\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5692\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5295\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4886\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5669\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6216\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6441\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5260\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3072\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7759\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6995\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6042\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6068\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3537\n",
      "Training loss over epoch: 1.5616\n",
      "Validation acc: 0.3358\n",
      "Validation loss: 1.6328\n",
      "Time taken: 227.67s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 1.5258\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.3602\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5335\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4948\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3964\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.3705\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4293\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5467\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5123\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4546\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1671\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6944\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6815\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4116\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5668\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4248\n",
      "Training loss over epoch: 1.4568\n",
      "Validation acc: 0.3358\n",
      "Validation loss: 1.6410\n",
      "Time taken: 228.37s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 1.4579\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.2788\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3689\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4278\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2856\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.3239\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2956\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5011\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3979\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3730\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.0842\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5598\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6322\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3067\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5289\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4737\n",
      "Training loss over epoch: 1.3665\n",
      "Validation acc: 0.3317\n",
      "Validation loss: 1.6665\n",
      "Time taken: 229.01s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 1.3741\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.2014\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2163\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3101\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2185\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2340\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2200\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4532\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2346\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3061\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.0164\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3862\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6012\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.1846\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4898\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5217\n",
      "Training loss over epoch: 1.2797\n",
      "Validation acc: 0.3383\n",
      "Validation loss: 1.6658\n",
      "Time taken: 224.57s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 1.2462\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.1282\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.0659\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2077\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1355\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.1814\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.1591\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4054\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.0776\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2290\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.9501\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2387\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5518\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.0682\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4364\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5610\n",
      "Training loss over epoch: 1.1984\n",
      "Validation acc: 0.3392\n",
      "Validation loss: 1.6777\n",
      "Time taken: 226.49s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 1.1335\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.0525\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.9820\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.0996\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.0540\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.1393\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.1116\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3599\n",
      "Seen so far: 2272 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 80: 0.9543\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.1550\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.8943\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1143\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4890\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9872\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3431\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5971\n",
      "Training loss over epoch: 1.1129\n",
      "Validation acc: 0.3475\n",
      "Validation loss: 1.6884\n",
      "Time taken: 226.03s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 1.0617\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.9820\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.9012\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.9956\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.9465\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.0740\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0727\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2833\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.8562\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.0896\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7984\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.9510\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4259\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9131\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2564\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6325\n",
      "Training loss over epoch: 1.0305\n",
      "Validation acc: 0.3375\n",
      "Validation loss: 1.7452\n",
      "Time taken: 224.89s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 0.9810\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.8990\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.8381\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.8908\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.8911\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.0272\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0472\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2203\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.7908\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.0257\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.8015\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.8200\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3655\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.8291\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.1626\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6692\n",
      "Training loss over epoch: 0.9549\n",
      "Validation acc: 0.3300\n",
      "Validation loss: 1.8210\n",
      "Time taken: 222.71s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 0.8698\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.8204\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7765\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.7804\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.8279\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.9804\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0104\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.1109\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.7131\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.1594\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3327\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.7252\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3711\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.8398\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.1100\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6712\n",
      "Training loss over epoch: 0.9475\n",
      "Validation acc: 0.3400\n",
      "Validation loss: 1.8785\n",
      "Time taken: 222.97s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 0.8789\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.8517\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7834\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.8340\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.7985\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.1384\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0024\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.1608\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.6848\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9846\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7501\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.6221\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2756\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.8132\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.0465\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6731\n",
      "Training loss over epoch: 0.9298\n",
      "Validation acc: 0.3367\n",
      "Validation loss: 1.8704\n",
      "Time taken: 224.46s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 0.7810\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.7039\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7081\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.6154\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.7233\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.8697\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9147\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.0009\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.6010\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.8682\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6272\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.4569\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2924\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.7405\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.9477\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7312\n",
      "Training loss over epoch: 0.7884\n",
      "Validation acc: 0.3308\n",
      "Validation loss: 2.0012\n",
      "Time taken: 226.21s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 0.6845\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.6365\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6971\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5275\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.8154\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.8505\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3819\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.9955\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.6904\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9565\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6164\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.4935\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1764\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.6634\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.8915\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7252\n",
      "Training loss over epoch: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.3217\n",
      "Validation loss: 2.4758\n",
      "Time taken: 222.62s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 0.8989\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.6512\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6854\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5562\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.6938\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.7643\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9071\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.8381\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.4636\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.1180\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6612\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.5306\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1428\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.6617\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.8584\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7379\n",
      "Training loss over epoch: 0.7612\n",
      "Validation acc: 0.3400\n",
      "Validation loss: 2.1022\n",
      "Time taken: 221.55s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 0.6503\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.5777\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5926\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5285\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.5736\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.7303\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.1413\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.8097\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.4668\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.6980\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.5791\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.3440\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.0136\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5668\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.7346\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7833\n",
      "Training loss over epoch: 0.6506\n",
      "Validation acc: 0.3358\n",
      "Validation loss: 2.2006\n",
      "Time taken: 225.30s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 0.5402\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.4733\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5495\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.2974\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4849\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.5995\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7496\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.6558\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.3096\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.6555\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.4791\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.2410\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.9190\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5220\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6570\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8283\n",
      "Training loss over epoch: 0.5353\n",
      "Validation acc: 0.3417\n",
      "Validation loss: 2.2846\n",
      "Time taken: 222.91s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 0.4737\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.3878\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5021\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.2279\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3869\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.5096\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.6593\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5864\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2827\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.5007\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.4424\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.2222\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.8295\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.4932\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5922\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8560\n",
      "Training loss over epoch: 0.4644\n",
      "Validation acc: 0.3408\n",
      "Validation loss: 2.4543\n",
      "Time taken: 225.95s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 0.4287\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.3476\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4414\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.1930\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3326\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.4620\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.5979\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5304\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2308\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.4507\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6502\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.5381\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.9347\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5738\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6601\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8438\n",
      "Training loss over epoch: 0.5006\n",
      "Validation acc: 0.3350\n",
      "Validation loss: 2.4601\n",
      "Time taken: 225.83s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 0.6269\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.7583\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6880\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5305\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4022\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.0983\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7998\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.0516\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.7127\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.6091\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.4599\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.2720\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.8552\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.6035\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6047\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7840\n",
      "Training loss over epoch: 0.6582\n",
      "Validation acc: 0.3475\n",
      "Validation loss: 2.4427\n",
      "Time taken: 223.79s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 0.4701\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.4267\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4830\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.2684\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4476\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.4687\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7397\n",
      "Seen so far: 1952 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 70: 0.5549\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2829\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.4341\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.4658\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.1869\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.7493\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.4410\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5259\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8452\n",
      "Training loss over epoch: 0.4797\n",
      "Validation acc: 0.3458\n",
      "Validation loss: 2.4911\n",
      "Time taken: 226.26s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 0.3568\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.3350\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4626\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.3319\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.2723\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.5038\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.6882\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5177\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2678\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.4800\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.3955\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.1303\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.7068\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3897\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5001\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8640\n",
      "Training loss over epoch: 0.4412\n",
      "Validation acc: 0.3642\n",
      "Validation loss: 2.5070\n",
      "Time taken: 225.34s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 0.3424\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.2797\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5153\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.1943\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.2658\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.4115\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.5506\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.4995\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2049\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.3766\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.3125\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0889\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.6025\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3290\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.3606\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8944\n",
      "Training loss over epoch: 0.3650\n",
      "Validation acc: 0.3533\n",
      "Validation loss: 2.6640\n",
      "Time taken: 223.54s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 0.2507\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.3110\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.2910\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.1229\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.1931\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.3239\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.4708\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.3830\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.1187\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.2530\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.2223\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0646\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.5204\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.2532\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.3144\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.9254\n",
      "Training loss over epoch: 0.2676\n",
      "Validation acc: 0.3300\n",
      "Validation loss: 2.9758\n",
      "Time taken: 224.15s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 0.2298\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.1501\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.2263\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.1053\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.1354\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.2725\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.4099\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.3174\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.0959\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.2142\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.1593\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0535\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.4696\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.2028\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.2853\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.9406\n",
      "Training loss over epoch: 0.2214\n",
      "Validation acc: 0.3442\n",
      "Validation loss: 3.1561\n",
      "Time taken: 223.90s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 0.1671\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.1185\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.2014\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.0837\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.1176\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.2252\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.3397\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.2834\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.0776\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.1928\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.1246\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0483\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.4254\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.1673\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.2541\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.9494\n",
      "Training loss over epoch: 0.1883\n",
      "Validation acc: 0.3292\n",
      "Validation loss: 3.3984\n",
      "Time taken: 223.22s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 0.1440\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.0864\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.1457\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.0721\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.0953\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.1907\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.2854\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.2510\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.0607\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.1589\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.1005\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0436\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.3834\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.1468\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.2175\n",
      "Seen so far: 4512 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch: 0.9592\n",
      "Training loss over epoch: 0.1602\n",
      "Validation acc: 0.3225\n",
      "Validation loss: 3.6077\n",
      "Time taken: 226.90s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 0.1173\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.1044\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.1756\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 3.4549\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.0321\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.8080\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3780\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.6346\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.4819\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.6085\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.3201\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.4181\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.6607\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3068\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.4617\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7987\n",
      "Training loss over epoch: 0.6950\n",
      "Validation acc: 0.3308\n",
      "Validation loss: 2.5220\n",
      "Time taken: 224.76s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 0.3272\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.4239\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4004\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.4972\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3284\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.4240\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.5281\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.3872\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2346\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.2759\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.1937\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.1412\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.4663\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.2193\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.3059\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8985\n",
      "Training loss over epoch: 0.3402\n",
      "Validation acc: 0.3450\n",
      "Validation loss: 2.7792\n",
      "Time taken: 224.50s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 0.1863\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.1796\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.2945\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.0998\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.1016\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.2206\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.2805\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.2591\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.0601\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.1658\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.0797\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0472\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.3926\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.1533\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.2133\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.9515\n",
      "Training loss over epoch: 0.1834\n",
      "Validation acc: 0.3275\n",
      "Validation loss: 3.0871\n",
      "Time taken: 224.55s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 0.1205\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 0.0719\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.1551\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.0692\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.0778\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.1881\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.2265\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.2053\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.0440\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.1222\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.0644\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.0343\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.3336\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.1075\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.1669\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.9685\n",
      "Training loss over epoch: 0.1361\n",
      "Validation acc: 0.3283\n",
      "Validation loss: 3.3776\n",
      "Time taken: 226.76s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1))\n",
    "    for step, batch in enumerate(training_batch):\n",
    "        x, y = preprocess_input(batch)\n",
    "        \n",
    "        loss = train_step(x, y)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc)))\n",
    "    loss_train = train_loss.result()\n",
    "    print(\"Training loss over epoch: %.4f\" % (float(loss_train)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    # For validation data:\n",
    "    for val_batch in validation_batch:\n",
    "        x_val, y_val = preprocess_input(val_batch)\n",
    "        \n",
    "        valid_step(x_val, y_val)\n",
    "        \n",
    "\n",
    "    # Metrics\n",
    "    val_acc = validation_metrics.result()\n",
    "    loss_val = validation_loss.result()\n",
    "    validation_metrics.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    # Append to a list for graph:\n",
    "    epoch_accuracy_train.append(train_acc)\n",
    "    epoch_accuracy_val.append(val_acc)\n",
    "    epoch_loss_train.append(loss_train)\n",
    "    epoch_loss_val.append(loss_val)\n",
    "    \n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "    print(\"Validation loss: %.4f\" % (float(loss_val)))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd8c0e",
   "metadata": {},
   "source": [
    "# Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ace2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = [i+1 for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87577487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_train, label = \"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Accuracy per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aeca8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = axis_x, y = epoch_loss_train, label=\"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_loss_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c65a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
