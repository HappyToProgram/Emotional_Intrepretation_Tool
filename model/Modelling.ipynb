{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe86bf",
   "metadata": {},
   "source": [
    "# Modelling:\n",
    "- This notebook uses the data obtained from Pre-Processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2674903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "tf.keras.utils.set_random_seed(442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 21:46:52.464299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 21:46:52.506712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 21:46:52.506803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# GPU Usage\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Set memory growth\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'Sadness': 0,\n",
    "    'Excited': 1,\n",
    "    'Happiness': 2,\n",
    "    'Anger' : 3,\n",
    "    'Frustration' : 4,\n",
    "    'Other' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf9f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    one_hot = np.zeros(6)\n",
    "    one_hot[labeldict[label]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(listOfLabels):\n",
    "    finalList = []\n",
    "    for label in listOfLabels:\n",
    "        finalList.append(one_hot_encode(label))\n",
    "    return np.array(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_and_label(path):\n",
    "    emotion = re.match('.*/DATA/([a-zA-Z]+)/.*', path).groups()[0]\n",
    "    data, _ = librosa.load(path, sr=44100)\n",
    "    STFT = np.abs(librosa.stft(data))\n",
    "    return STFT, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(pathList): # Returns a list of x (batch_size, timesteps, feature), y (one_hot_encoded)\n",
    "    with mp.Pool() as p:\n",
    "        results = p.map(get_STFT_and_label, pathList)\n",
    "    # Preprocess x:\n",
    "    x = [item[0] for item in results]\n",
    "    # Flatten\n",
    "    x = [item for sublist in x for item in sublist]\n",
    "    # Zero-padding:\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", maxlen=1497, dtype = np.float32) # maxlen is after discovering the whole training data\n",
    "    # Reshaping so that the order is not messed up\n",
    "    x = x.reshape(-1, 1025, 1497)\n",
    "    # Transposing so that we have timesteps in dim 1\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    # Preprocess y:\n",
    "    y = [item[1] for item in results]\n",
    "    # one_hot_encode\n",
    "    y = one_hot_encode_list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5000",
   "metadata": {},
   "source": [
    "# Loading data: \n",
    "- We will load the data per predefined batch size, this is to reduce the memory used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fbf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_paths.pkl', 'rb') as f:\n",
    "    train_paths = pickle.load(f)\n",
    "with open('test_paths.pkl', 'rb') as f:\n",
    "    test_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batches of the pathList:\n",
    "def create_batches(pathList, batch_size):\n",
    "    ansList = [] # To store the final batched paths\n",
    "    tempList = [] # Temporary list\n",
    "    count = 0\n",
    "    while count < len(pathList):\n",
    "        tempList.append(pathList[count]) # Append the path\n",
    "        count += 1\n",
    "        if (count % batch_size) == 0: # if count is a multiple of batch_size\n",
    "            ansList.append(tempList)\n",
    "            tempList = []\n",
    "    if len(tempList) != 0: # If tempList is not empty\n",
    "        ansList.append(tempList) # Append the remaining values\n",
    "    return ansList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e7d8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation datasets. The validation datasets are loaded entirely to the machine.\n",
    "x_val, y_val = preprocess_input(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08f75e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1489, 1025)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28921b58",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "920ba4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 22:00:20.048090: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-21 22:00:20.049762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 22:00:20.049956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 22:00:20.050078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 22:00:20.529942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 22:00:20.530120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 22:00:20.530258: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-21 22:00:20.530527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6128 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Keras API:\n",
    "inp = layers.Input(shape=(None, 1025))\n",
    "x = layers.Masking(mask_value=0.0)(inp)\n",
    "total_seq, final_hidden_state, final_cell_state = layers.LSTM(256, return_state=True)(x)\n",
    "x = layers.Dense(512, activation='relu')(final_hidden_state)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3908fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 1025)]      0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 1025)        0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 [(None, 256),             1312768   \n",
      "                              (None, 256),                       \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,609,350\n",
      "Trainable params: 1,609,350\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b122fa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28347024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size is 32, epochs = 30\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer is Stochastic Gradient Descent\n",
    "# Loss function is Categorical Crossentropy\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc25bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = create_batches(train_paths, batch_size=batch_size)\n",
    "validation_batch = create_batches(test_paths, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3369e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.CategoricalCrossentropy()\n",
    "validation_loss = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "538ba3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store epoch results:\n",
    "epoch_accuracy_train = []\n",
    "epoch_accuracy_val = []\n",
    "epoch_loss_train = []\n",
    "epoch_loss_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f3c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up, use graph execution\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training = True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # Update training accuracy\n",
    "    train_metrics.update_state(y, y_pred)\n",
    "    # Update training loss:\n",
    "    train_loss.update_state(y, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a850e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(x, y):\n",
    "    y_val_pred = model(x, training=False)\n",
    "    # Update metrics for validation\n",
    "    validation_metrics.update_state(y, y_val_pred)\n",
    "    validation_loss.update_state(y, y_val_pred)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1971ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-21 22:00:46.502635: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8400\n",
      "2022-05-21 22:00:46.633603: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 1.7829\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7884\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7899\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7747\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7946\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8121\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7827\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7912\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7901\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7842\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7758\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7939\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7759\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7981\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7494\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2052\n",
      "Training loss over epoch: 1.7849\n",
      "Validation acc: 0.2183\n",
      "Validation loss: 1.7752\n",
      "Time taken: 352.04s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.7501\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7872\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7628\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7431\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7740\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7914\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7583\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7791\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7820\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7626\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7582\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7794\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7701\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7836\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7248\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2488\n",
      "Training loss over epoch: 1.7663\n",
      "Validation acc: 0.2450\n",
      "Validation loss: 1.7641\n",
      "Time taken: 426.11s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.7255\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7859\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7463\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7225\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7563\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7740\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7371\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7650\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7670\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7507\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7437\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7668\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7719\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7682\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7152\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2640\n",
      "Training loss over epoch: 1.7522\n",
      "Validation acc: 0.2633\n",
      "Validation loss: 1.7555\n",
      "Time taken: 413.83s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 1.7095\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7842\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7354\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7080\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7456\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7627\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7197\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7536\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7512\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7407\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7314\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7576\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7728\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7548\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7084\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2769\n",
      "Training loss over epoch: 1.7408\n",
      "Validation acc: 0.2833\n",
      "Validation loss: 1.7490\n",
      "Time taken: 376.49s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 1.6985\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7828\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7285\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6961\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7384\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7578\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7062\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7457\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7350\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7324\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7214\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7513\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7703\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7435\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7034\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2829\n",
      "Training loss over epoch: 1.7312\n",
      "Validation acc: 0.2867\n",
      "Validation loss: 1.7436\n",
      "Time taken: 377.69s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 1.6901\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7822\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7232\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6856\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7332\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7563\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6951\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7405\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7198\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7254\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7120\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7472\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7652\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7326\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6999\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2854\n",
      "Training loss over epoch: 1.7221\n",
      "Validation acc: 0.2933\n",
      "Validation loss: 1.7385\n",
      "Time taken: 377.36s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 1.6831\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7830\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7173\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6744\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7274\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7574\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6860\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7389\n",
      "Seen so far: 2272 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 80: 1.7042\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7184\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7025\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7451\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7585\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7235\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6971\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2883\n",
      "Training loss over epoch: 1.7129\n",
      "Validation acc: 0.2942\n",
      "Validation loss: 1.7333\n",
      "Time taken: 377.24s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 1.6769\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7854\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7116\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6634\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7202\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7610\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6790\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7385\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6885\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7119\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6928\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7468\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7486\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7150\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6941\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2915\n",
      "Training loss over epoch: 1.7032\n",
      "Validation acc: 0.2900\n",
      "Validation loss: 1.7279\n",
      "Time taken: 377.78s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 1.6705\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7895\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7048\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6515\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7127\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7671\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6745\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7407\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6746\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7058\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6821\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7521\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7361\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7073\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6904\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2994\n",
      "Training loss over epoch: 1.6930\n",
      "Validation acc: 0.2808\n",
      "Validation loss: 1.7229\n",
      "Time taken: 379.20s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 1.6655\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7950\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6968\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6403\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7052\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7749\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6709\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7440\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6611\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7025\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6717\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7612\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7248\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7004\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6868\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3021\n",
      "Training loss over epoch: 1.6824\n",
      "Validation acc: 0.2783\n",
      "Validation loss: 1.7184\n",
      "Time taken: 384.33s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 1.6606\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8019\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6863\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6291\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6972\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7848\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6689\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7465\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6465\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7032\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6607\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7730\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7133\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6938\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6828\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3042\n",
      "Training loss over epoch: 1.6715\n",
      "Validation acc: 0.2817\n",
      "Validation loss: 1.7140\n",
      "Time taken: 382.58s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 1.6564\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8085\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6727\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6186\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6869\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7932\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6694\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7474\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6286\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7101\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6459\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7837\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7011\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6859\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6735\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3077\n",
      "Training loss over epoch: 1.6600\n",
      "Validation acc: 0.2792\n",
      "Validation loss: 1.7092\n",
      "Time taken: 374.91s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 1.6473\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8158\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6523\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6047\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6703\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8011\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6696\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7455\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6008\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7227\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6264\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7842\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6875\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6758\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6554\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3156\n",
      "Training loss over epoch: 1.6464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.2733\n",
      "Validation loss: 1.7008\n",
      "Time taken: 381.18s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 1.6334\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8169\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6228\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5836\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6368\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8109\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6657\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7355\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5586\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7388\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5990\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7599\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6783\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6665\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6230\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3215\n",
      "Training loss over epoch: 1.6299\n",
      "Validation acc: 0.2792\n",
      "Validation loss: 1.6924\n",
      "Time taken: 379.48s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 1.6608\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7926\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6071\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5693\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5970\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8129\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6311\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7171\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5349\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7317\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5690\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7369\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7133\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6609\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5943\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3285\n",
      "Training loss over epoch: 1.6142\n",
      "Validation acc: 0.2883\n",
      "Validation loss: 1.6844\n",
      "Time taken: 377.74s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 1.6567\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8364\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6015\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5887\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5670\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8026\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5842\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6813\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5229\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6907\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5496\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7244\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7658\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6518\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5750\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3333\n",
      "Training loss over epoch: 1.5991\n",
      "Validation acc: 0.2925\n",
      "Validation loss: 1.6635\n",
      "Time taken: 382.28s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 1.5942\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8661\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5927\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5881\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5126\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7749\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5587\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6667\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5020\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6720\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5644\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7049\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.8257\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6393\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5740\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3417\n",
      "Training loss over epoch: 1.5866\n",
      "Validation acc: 0.2942\n",
      "Validation loss: 1.6617\n",
      "Time taken: 380.46s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 1.5589\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7713\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5721\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5483\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4527\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7383\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5454\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6448\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5891\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6537\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5815\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7108\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6811\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6499\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5702\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3527\n",
      "Training loss over epoch: 1.5683\n",
      "Validation acc: 0.3042\n",
      "Validation loss: 1.6627\n",
      "Time taken: 381.01s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 1.6279\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7403\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6262\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5376\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4383\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7404\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5619\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5922\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4561\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6485\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6041\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6842\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.8636\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5880\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5528\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3523\n",
      "Training loss over epoch: 1.5581\n",
      "Validation acc: 0.2975\n",
      "Validation loss: 1.6384\n",
      "Time taken: 380.73s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 1.5427\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7262\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5671\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5054\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4086\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6815\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5703\n",
      "Seen so far: 1952 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 70: 1.5892\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5230\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6297\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5222\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6639\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6581\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5791\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5454\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3640\n",
      "Training loss over epoch: 1.5385\n",
      "Validation acc: 0.3083\n",
      "Validation loss: 1.6169\n",
      "Time taken: 380.08s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 1.5236\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7193\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5752\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4786\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4572\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6751\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5314\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5708\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4998\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7043\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5394\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6661\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6180\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5986\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5285\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3631\n",
      "Training loss over epoch: 1.5397\n",
      "Validation acc: 0.2875\n",
      "Validation loss: 1.6735\n",
      "Time taken: 379.65s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 1.6333\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7280\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5638\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4497\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4169\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6501\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4845\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7937\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4788\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5935\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4711\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6622\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5759\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5770\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5280\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3671\n",
      "Training loss over epoch: 1.5205\n",
      "Validation acc: 0.3150\n",
      "Validation loss: 1.6190\n",
      "Time taken: 380.14s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 1.5093\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7853\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5452\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4367\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3827\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6656\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5182\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6250\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5077\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5721\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5481\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6407\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5632\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5475\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5143\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3810\n",
      "Training loss over epoch: 1.5083\n",
      "Validation acc: 0.3000\n",
      "Validation loss: 1.6248\n",
      "Time taken: 380.26s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 1.4605\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6959\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5649\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4297\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4348\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6223\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4942\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5408\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4338\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5291\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6647\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6130\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5840\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5320\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7043\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3671\n",
      "Training loss over epoch: 1.5219\n",
      "Validation acc: 0.2300\n",
      "Validation loss: 1.7086\n",
      "Time taken: 377.58s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 1.6354\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8270\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6275\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5694\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5480\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6730\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6613\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7040\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5017\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6889\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5890\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6752\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6516\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6264\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5879\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3327\n",
      "Training loss over epoch: 1.5874\n",
      "Validation acc: 0.2975\n",
      "Validation loss: 1.6786\n",
      "Time taken: 380.92s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 1.6156\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8522\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5901\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5343\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5131\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6696\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7414\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6937\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5138\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6791\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4837\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6329\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6446\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5738\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5589\n",
      "Seen so far: 4512 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch: 0.3550\n",
      "Training loss over epoch: 1.5506\n",
      "Validation acc: 0.3083\n",
      "Validation loss: 1.6589\n",
      "Time taken: 379.20s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 1.5774\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8466\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5616\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4898\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4533\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6791\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6061\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6790\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5020\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6607\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4426\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6618\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6789\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5396\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5205\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3721\n",
      "Training loss over epoch: 1.5240\n",
      "Validation acc: 0.3075\n",
      "Validation loss: 1.6597\n",
      "Time taken: 378.04s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 1.5464\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7806\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5670\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4668\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3981\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7144\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6556\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5853\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4503\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6163\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4746\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6252\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5565\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5317\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4825\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3852\n",
      "Training loss over epoch: 1.4942\n",
      "Validation acc: 0.3300\n",
      "Validation loss: 1.6349\n",
      "Time taken: 381.09s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 1.5144\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8275\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6137\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4419\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3466\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5695\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5122\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4938\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4740\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5287\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3968\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6103\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5885\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4972\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4711\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3998\n",
      "Training loss over epoch: 1.4658\n",
      "Validation acc: 0.3333\n",
      "Validation loss: 1.5939\n",
      "Time taken: 379.33s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 1.5146\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6866\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5677\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4255\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3378\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5436\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5334\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4914\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4161\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5256\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5525\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6015\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7339\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4898\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4723\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3923\n",
      "Training loss over epoch: 1.4776\n",
      "Validation acc: 0.3433\n",
      "Validation loss: 1.6154\n",
      "Time taken: 382.02s\n"
     ]
    }
   ],
   "source": [
    "# Custom Training loop:\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch))\n",
    "    for step, batch in enumerate(training_batch):\n",
    "        x, y = preprocess_input(batch)\n",
    "        \n",
    "        loss = train_step(x, y)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc)))\n",
    "    loss_train = train_loss.result()\n",
    "    print(\"Training loss over epoch: %.4f\" % (float(loss_train)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    # For validation data:\n",
    "    for val_batch in validation_batch:\n",
    "        x_val, y_val = preprocess_input(val_batch)\n",
    "        \n",
    "        valid_step(x_val, y_val)\n",
    "        \n",
    "\n",
    "    # Metrics\n",
    "    val_acc = validation_metrics.result()\n",
    "    loss_val = validation_loss.result()\n",
    "    validation_metrics.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    # Append to a list for graph:\n",
    "    epoch_accuracy_train.append(train_acc)\n",
    "    epoch_accuracy_val.append(val_acc)\n",
    "    epoch_loss_train.append(loss_train)\n",
    "    epoch_loss_val.append(loss_val)\n",
    "    \n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "    print(\"Validation loss: %.4f\" % (float(loss_val)))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd8c0e",
   "metadata": {},
   "source": [
    "# Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = [i+1 for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87577487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_train, label = \"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Accuracy per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeca8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = axis_x, y = epoch_loss_train, label=\"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_loss_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c65a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
