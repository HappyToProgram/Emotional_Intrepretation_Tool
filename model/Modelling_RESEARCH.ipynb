{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe86bf",
   "metadata": {},
   "source": [
    "# Modelling with some reference to the article found in arxiv:\n",
    "- This notebook uses the data obtained from Pre-Processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2674903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "tf.keras.utils.set_random_seed(442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 09:05:32.560970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.566442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.566542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# GPU Usage\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Set memory growth\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'Sadness': 0,\n",
    "    'Excited': 1,\n",
    "    'Happiness': 2,\n",
    "    'Anger' : 3,\n",
    "    'Frustration' : 4,\n",
    "    'Other' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf9f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    one_hot = np.zeros(6)\n",
    "    one_hot[labeldict[label]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(listOfLabels):\n",
    "    finalList = []\n",
    "    for label in listOfLabels:\n",
    "        finalList.append(one_hot_encode(label))\n",
    "    return np.array(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_and_label(path):\n",
    "    emotion = re.match('.*/DATA/([a-zA-Z]+)/.*', path).groups()[0]\n",
    "    data, _ = librosa.load(path, sr=44100)\n",
    "    STFT = np.abs(librosa.stft(data))\n",
    "    return STFT, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(pathList): # Returns a list of x (batch_size, timesteps, feature), y (one_hot_encoded)\n",
    "    with mp.Pool() as p:\n",
    "        results = p.map(get_STFT_and_label, pathList)\n",
    "    # Preprocess x:\n",
    "    x = [item[0] for item in results]\n",
    "    # Flatten\n",
    "    x = [item for sublist in x for item in sublist]\n",
    "    # Zero-padding:\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", maxlen=1497, dtype = np.float32) # maxlen is after discovering the whole training data\n",
    "    # Reshaping so that the order is not messed up\n",
    "    x = x.reshape(-1, 1025, 1497)\n",
    "    # Transposing so that we have timesteps in dim 1\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    # For 3D CNN we need to make it in the shape of (batch, height, width, channel)\n",
    "    x = tf.expand_dims(x, 3)\n",
    "    # Preprocess y:\n",
    "    y = [item[1] for item in results]\n",
    "    # one_hot_encode\n",
    "    y = one_hot_encode_list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5000",
   "metadata": {},
   "source": [
    "# Loading data: \n",
    "- We will load the data per predefined batch size, this is to reduce the memory used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fbf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_paths.pkl', 'rb') as f:\n",
    "    train_paths = pickle.load(f)\n",
    "with open('test_paths.pkl', 'rb') as f:\n",
    "    test_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batches of the pathList:\n",
    "def create_batches(pathList, batch_size):\n",
    "    ansList = [] # To store the final batched paths\n",
    "    tempList = [] # Temporary list\n",
    "    count = 0\n",
    "    while count < len(pathList):\n",
    "        tempList.append(pathList[count]) # Append the path\n",
    "        count += 1\n",
    "        if (count % batch_size) == 0: # if count is a multiple of batch_size\n",
    "            ansList.append(tempList)\n",
    "            tempList = []\n",
    "    if len(tempList) != 0: # If tempList is not empty\n",
    "        ansList.append(tempList) # Append the remaining values\n",
    "    return ansList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28921b58",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920ba4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 09:05:32.597855: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-27 09:05:32.598510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.598643: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.598707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.908617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.908844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.908912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-27 09:05:32.908977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5235 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Keras API:\n",
    "inp = layers.Input(shape=(1497, 1025, 1)) # Let's make to fixed so that we could use CNN more efficiently\n",
    "\n",
    "# CNN part:\n",
    "\n",
    "# first:\n",
    "x = layers.Conv2D(16, kernel_size=(12,8), padding='same', strides=8)(inp)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu and not relu (See Dying Relu problem that leads to overfitting)\n",
    "x = layers.MaxPool2D(pool_size=(2,2), padding='same')(x)\n",
    "\n",
    "# second:\n",
    "x = layers.Conv2D(32, kernel_size=(8,6), padding='same', strides=6)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu \n",
    "x = layers.MaxPool2D(pool_size=(2,2), padding='same')(x)\n",
    "\n",
    "# Third\n",
    "x = layers.Conv2D(32, kernel_size=(3,3), padding='same', strides=3)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu\n",
    "x = layers.MaxPool2D(pool_size=(2,2), padding='same')(x)\n",
    "\n",
    "# # Flatten\n",
    "# x = layers.Flatten()(x)\n",
    "\n",
    "# Reduce all dim 2 of size 1 so that it can be input to RNN: (batch, timestep, feature)\n",
    "# Later Think about concat where the RNN below is simply taking the input from inp\n",
    "x = tf.squeeze(x, axis=2)\n",
    "\n",
    "# RNN\n",
    "seq1, hidden_state1 = layers.SimpleRNN(256, return_state=True)(x)\n",
    "seq2, hidden_state2 = layers.SimpleRNN(256, return_state=True)(x, initial_state=hidden_state1) # USe previous RNN's states to initialize\n",
    "\n",
    "x = layers.Dense(128, activation='relu')(hidden_state2)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3908fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1497, 1025,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 188, 129, 16  1552        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 188, 129, 16  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 94, 65, 16)   0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 16, 11, 32)   24608       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16, 11, 32)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 8, 6, 32)    0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 3, 2, 32)     9248        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 3, 2, 32)     0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 2, 1, 32)    0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  (None, 2, 32)       0           ['max_pooling2d_2[0][0]']        \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " simple_rnn (SimpleRNN)         [(None, 256),        73984       ['tf.compat.v1.squeeze[0][0]']   \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " simple_rnn_1 (SimpleRNN)       [(None, 256),        73984       ['tf.compat.v1.squeeze[0][0]',   \n",
      "                                 (None, 256)]                     'simple_rnn[0][1]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          32896       ['simple_rnn_1[0][1]']           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6)            390         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 224,918\n",
      "Trainable params: 224,918\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b122fa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28347024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size is 32, epochs = 30\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer is Stochastic Gradient Descent\n",
    "# Loss function is Categorical Crossentropy\n",
    "optimizer = keras.optimizers.Adam() #amsgrad=False\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc25bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = create_batches(train_paths, batch_size=batch_size)\n",
    "validation_batch = create_batches(test_paths, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3369e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.CategoricalCrossentropy()\n",
    "validation_loss = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538ba3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store epoch results:\n",
    "epoch_accuracy_train = []\n",
    "epoch_accuracy_val = []\n",
    "epoch_loss_train = []\n",
    "epoch_loss_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up, use graph execution\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training = True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # Update training accuracy\n",
    "    train_metrics.update_state(y, y_pred)\n",
    "    # Update training loss:\n",
    "    train_loss.update_state(y, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a850e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(x, y):\n",
    "    y_val_pred = model(x, training=False)\n",
    "    # Update metrics for validation\n",
    "    validation_metrics.update_state(y, y_val_pred)\n",
    "    validation_loss.update_state(y, y_val_pred)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1971ed86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 09:05:36.407762: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8201\n",
      "2022-05-27 09:05:38.420904: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 10: 1.7406\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7271\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7304\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.8665\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7301\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7301\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6306\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7661\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7199\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.8054\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5424\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7210\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6458\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7864\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2419\n",
      "Training loss over epoch: 1.7002\n",
      "Validation acc: 0.3000\n",
      "Validation loss: 1.6336\n",
      "Time taken: 365.89s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 10: 1.4826\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6010\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5771\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.8565\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6177\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7160\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5290\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5711\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6336\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6450\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5794\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5897\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5512\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5859\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2910\n",
      "Training loss over epoch: 1.6315\n",
      "Validation acc: 0.3150\n",
      "Validation loss: 1.6200\n",
      "Time taken: 362.34s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 10: 1.7116\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6805\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4115\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6010\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7362\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.8287\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5053\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6269\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6162\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7041\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7573\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7441\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6988\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6564\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3260\n",
      "Training loss over epoch: 1.5936\n",
      "Validation acc: 0.3117\n",
      "Validation loss: 1.6177\n",
      "Time taken: 367.64s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 10: 1.6904\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6042\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7761\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3172\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4243\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7267\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6558\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4408\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4913\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5942\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7480\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4060\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4927\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6615\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3425\n",
      "Training loss over epoch: 1.5590\n",
      "Validation acc: 0.3292\n",
      "Validation loss: 1.6082\n",
      "Time taken: 364.59s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 10: 1.3448\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3147\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2573\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5896\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5701\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6039\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4783\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7033\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4807\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5918\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5441\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4939\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7401\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6484\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3681\n",
      "Training loss over epoch: 1.4867\n",
      "Validation acc: 0.3225\n",
      "Validation loss: 1.6077\n",
      "Time taken: 365.27s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 10: 1.4323\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4043\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2880\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1837\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.3578\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3913\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.8078\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5065\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5074\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6822\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4200\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3120\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4223\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5113\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4019\n",
      "Training loss over epoch: 1.4336\n",
      "Validation acc: 0.2975\n",
      "Validation loss: 1.6438\n",
      "Time taken: 364.87s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 10: 1.3090\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2741\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3707\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6051\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4132\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2967\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4100\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3811\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4086\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4122\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3399\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1558\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4724\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3378\n",
      "Seen so far: 4512 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch: 0.4400\n",
      "Training loss over epoch: 1.3668\n",
      "Validation acc: 0.3358\n",
      "Validation loss: 1.6790\n",
      "Time taken: 359.73s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 10: 1.3578\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.1293\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5161\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.0679\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5169\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0710\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3220\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3621\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2465\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1953\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2575\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3052\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.1775\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4489\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4579\n",
      "Training loss over epoch: 1.3076\n",
      "Validation acc: 0.3375\n",
      "Validation loss: 1.6871\n",
      "Time taken: 361.86s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 10: 1.0829\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.0662\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2877\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4463\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2087\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0164\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2011\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2415\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2186\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1069\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1808\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2986\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.1925\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2787\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4842\n",
      "Training loss over epoch: 1.2323\n",
      "Validation acc: 0.3083\n",
      "Validation loss: 1.8322\n",
      "Time taken: 363.85s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 10: 1.4039\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.8437\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.0964\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.9772\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.1698\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9850\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2570\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3184\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.1256\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1439\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1535\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2143\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.1644\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3349\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5179\n",
      "Training loss over epoch: 1.1647\n",
      "Validation acc: 0.3325\n",
      "Validation loss: 2.0148\n",
      "Time taken: 363.80s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 10: 1.0443\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.9107\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.0182\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.0381\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2561\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9270\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.0386\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4130\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.1751\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.0532\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1572\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1961\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3232\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.0833\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5519\n",
      "Training loss over epoch: 1.0714\n",
      "Validation acc: 0.3158\n",
      "Validation loss: 2.2306\n",
      "Time taken: 365.82s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 10: 1.0255\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6250\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.9918\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1767\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.1221\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9773\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.0262\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.8824\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9619\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7433\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1134\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1170\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.1066\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3369\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5669\n",
      "Training loss over epoch: 1.0151\n",
      "Validation acc: 0.3275\n",
      "Validation loss: 2.2523\n",
      "Time taken: 361.25s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 10: 0.7810\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7807\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.0108\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.8106\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.0366\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9559\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.9391\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.8749\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.0474\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7528\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.8333\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.7928\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.8419\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.0782\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5996\n",
      "Training loss over epoch: 0.9559\n",
      "Validation acc: 0.3167\n",
      "Validation loss: 2.4558\n",
      "Time taken: 359.62s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 10: 0.7608\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7342\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4178\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.7829\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.8536\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9730\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.8045\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.8844\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3246\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.8381\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.0078\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.9559\n",
      "Seen so far: 3872 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 130: 0.9215\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5986\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6321\n",
      "Training loss over epoch: 0.8930\n",
      "Validation acc: 0.3133\n",
      "Validation loss: 2.5682\n",
      "Time taken: 366.38s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 10: 0.7120\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7178\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2836\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.7221\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.8692\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7518\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.6838\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.8856\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.5754\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6979\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.0051\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.9245\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9220\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.8916\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6413\n",
      "Training loss over epoch: 0.8428\n",
      "Validation acc: 0.3058\n",
      "Validation loss: 2.9368\n",
      "Time taken: 363.95s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 10: 0.5520\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5441\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.6819\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.8657\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.5898\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9113\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.4846\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.9496\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.7792\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7511\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.0483\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.5434\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.7564\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2096\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6612\n",
      "Training loss over epoch: 0.7920\n",
      "Validation acc: 0.3075\n",
      "Validation loss: 2.6445\n",
      "Time taken: 366.52s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 10: 0.6366\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6870\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.7274\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.9274\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.8217\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.8765\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.9052\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.7284\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.7327\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7304\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.8433\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.7023\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.8458\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6434\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6848\n",
      "Training loss over epoch: 0.7397\n",
      "Validation acc: 0.3083\n",
      "Validation loss: 3.0357\n",
      "Time taken: 368.36s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 10: 1.0443\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6788\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.0719\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.8223\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.9950\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7611\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.7505\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.8676\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.6835\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6189\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.6920\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.7778\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5922\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.7746\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.6971\n",
      "Training loss over epoch: 0.7150\n",
      "Validation acc: 0.3183\n",
      "Validation loss: 3.4272\n",
      "Time taken: 366.19s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 10: 0.6668\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.7392\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.4867\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.5446\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.6033\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7412\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.6380\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.9347\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.7546\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.7852\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.6484\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.7957\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5821\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5902\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7025\n",
      "Training loss over epoch: 0.7005\n",
      "Validation acc: 0.3017\n",
      "Validation loss: 3.2150\n",
      "Time taken: 365.58s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 10: 0.5616\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4559\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.6444\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4836\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.0121\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.6080\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5754\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.7559\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9045\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6423\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.6889\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.5486\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.0994\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6550\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7237\n",
      "Training loss over epoch: 0.6507\n",
      "Validation acc: 0.3283\n",
      "Validation loss: 3.4638\n",
      "Time taken: 365.49s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 10: 0.4851\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5206\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5512\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.7015\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.6186\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.7523\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.4736\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.5893\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.5750\n",
      "Seen so far: 2912 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 100: 0.7238\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.7803\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.6327\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9748\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6288\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7350\n",
      "Training loss over epoch: 0.6305\n",
      "Validation acc: 0.2950\n",
      "Validation loss: 3.4872\n",
      "Time taken: 370.15s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 10: 0.8138\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.9634\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.4254\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4925\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.5345\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.5183\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.8465\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.5154\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9022\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6522\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.6187\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.5005\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9377\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5991\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7383\n",
      "Training loss over epoch: 0.6250\n",
      "Validation acc: 0.3108\n",
      "Validation loss: 3.2473\n",
      "Time taken: 365.79s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 10: 0.5927\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4185\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.0636\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4692\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.4710\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.4684\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5732\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.6787\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.7626\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.5219\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.4821\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.3966\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5480\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.8402\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7573\n",
      "Training loss over epoch: 0.5982\n",
      "Validation acc: 0.3092\n",
      "Validation loss: 3.1043\n",
      "Time taken: 365.77s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 10: 0.2972\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.5092\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.3873\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3599\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.6334\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.8017\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5467\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.6230\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.6623\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.6701\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.4090\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.3875\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.6569\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.4193\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7744\n",
      "Training loss over epoch: 0.5424\n",
      "Validation acc: 0.3083\n",
      "Validation loss: 3.3859\n",
      "Time taken: 366.46s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 10: 0.2879\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.3974\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5064\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.7475\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.5268\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.4573\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.4598\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.5930\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.5464\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.3002\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.6670\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.4833\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3334\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.4207\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.7898\n",
      "Training loss over epoch: 0.5087\n",
      "Validation acc: 0.3050\n",
      "Validation loss: 3.5447\n",
      "Time taken: 369.66s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 10: 0.4698\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4730\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.4647\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4211\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.4897\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.2213\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.4086\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.3703\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.5321\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.3662\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.3512\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.6217\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3715\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6618\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8031\n",
      "Training loss over epoch: 0.4705\n",
      "Validation acc: 0.3100\n",
      "Validation loss: 4.0675\n",
      "Time taken: 367.33s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 10: 0.6820\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.2630\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.3236\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.4998\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.7182\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.5544\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.6794\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.4034\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.5076\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.2949\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.7635\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.5341\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3767\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.6751\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8121\n",
      "Training loss over epoch: 0.4761\n",
      "Validation acc: 0.3142\n",
      "Validation loss: 4.0589\n",
      "Time taken: 367.06s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 10: 0.6178\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.4478\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.5219\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3134\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.3652\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.3420\n",
      "Seen so far: 1952 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 70: 0.3823\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.4121\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.2598\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.4210\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.2655\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.5660\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.5520\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.5543\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8235\n",
      "Training loss over epoch: 0.4174\n",
      "Validation acc: 0.2875\n",
      "Validation loss: 4.2905\n",
      "Time taken: 367.58s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 10: 0.4930\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.1732\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.2754\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3498\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.3684\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.2959\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.2001\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.2177\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.3008\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.3372\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.5764\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.1412\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3615\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.2354\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8413\n",
      "Training loss over epoch: 0.3925\n",
      "Validation acc: 0.3117\n",
      "Validation loss: 3.9178\n",
      "Time taken: 368.55s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 10: 0.4825\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.6759\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.2446\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.3688\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 0.2887\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.3425\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 0.5683\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.3667\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.3656\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 0.4738\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 0.4178\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 0.3999\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.3674\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 0.2259\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.8419\n",
      "Training loss over epoch: 0.4069\n",
      "Validation acc: 0.3058\n",
      "Validation loss: 4.3806\n",
      "Time taken: 367.62s\n"
     ]
    }
   ],
   "source": [
    "# Custom Training loop:\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch))\n",
    "    # Shuffle the training batch for each epoch:\n",
    "    random.shuffle(training_batch)\n",
    "    for step, batch in enumerate(training_batch):\n",
    "        x, y = preprocess_input(batch)\n",
    "        \n",
    "        loss = train_step(x, y)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 10 == 0 and step != 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc)))\n",
    "    loss_train = train_loss.result()\n",
    "    print(\"Training loss over epoch: %.4f\" % (float(loss_train)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    # For validation data:\n",
    "    for val_batch in validation_batch:\n",
    "        x_val, y_val = preprocess_input(val_batch)\n",
    "        \n",
    "        valid_step(x_val, y_val)\n",
    "        \n",
    "\n",
    "    # Metrics\n",
    "    val_acc = validation_metrics.result()\n",
    "    loss_val = validation_loss.result()\n",
    "    validation_metrics.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    # Append to a list for graph:\n",
    "    epoch_accuracy_train.append(train_acc)\n",
    "    epoch_accuracy_val.append(val_acc)\n",
    "    epoch_loss_train.append(loss_train)\n",
    "    epoch_loss_val.append(loss_val)\n",
    "    \n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "    print(\"Validation loss: %.4f\" % (float(loss_val)))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd8c0e",
   "metadata": {},
   "source": [
    "# Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ace2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = [i+1 for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87577487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAF1CAYAAADMXG9eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzo0lEQVR4nO3dfZxcZX3w/8+XJcBCYhaJRthEktuHqCEhS2i0ptpErUHuWiNFjSJqLfLSSqwP5IbcPrf1JjatWlotv9aidys2UgirpdBYIautFYUQyAOYG1SUbHgQakICS0nC9ftjzobJsg8zc87unJ39vF+veWXOw3znO7NX5nznOtdcJ1JKSJIkqTFHNDsBSZKk8cxiSpIkKQeLKUmSpBwspiRJknKwmJIkScrBYkqSJCkHiylJKrmI+GpE/Emz85A0OIspaYKJiJ6I+FVEHN3sXCSpFVhMSRNIRMwCXgEk4HfG+LmPHMvnK8J4zFnS2LOYkiaWdwA3AV8F3lm9ISJmRsT6iPhlRDwcEX9Vte09EXFnROyNiDsi4rRsfYqI51ftd+h0VEQsiYidEXFRRNwPfCUijo+Ia7Pn+FV2f0bV458ZEV+JiF3Z9u5s/baIeH3VfpMi4qGIWDDwBVY97//O9rknIs6p2n50RPxZRPwiIh6IiMsion2onAd7EyPi3dn78auI2BARJ1dtSxHxgYj4afb8ayPiiGzbERHxsYj4eUQ8GBF/HxFTqx77GxHxnxGxOyLujYh3VT3t8RHxL9nf4IcR8bzBcpM09iympInlHcAV2W1ZREwHiIg24Frg58AsoBNYl217E/Cp7LHPoNKj9XCNz/cc4JnAycD5VD5zvpItPxfoA/6qav9/AI4F5gLPBj6frf974O1V+50J3JdSum2Y552WvY53An8TEXOybZ8FXggsAJ6f7fOJYXI+TEQsB/43cBbwLODfgX8csNsbgdOB04A3AO/O1r8ruy0F/gcwuf/1R8RzgeuBv8ziLgCqX99bgU8DxwN3A58Z4rVLGmspJW/evE2AG/AbwH5gWrb8Y+BD2f1fB34JHDnI4zYAfzhEzAQ8v2r5q8CfZPeXAE8AxwyT0wLgV9n9E4EngeMH2e8kYC/wjGz5KuB/DRFzCXAAOK5q3ZXAx4EAHgWeV7Xt14Gf1ZHz9cDvVy0fATwGnFz1npxRtf0PgBuy+zcAf1C1bU72NzkSWA1cM8RzfhX4ctXymcCPm92mvHnzVrnZMyVNHO8Evp1Seihb/jpPneqbCfw8pXRgkMfNBH7S4HP+MqX0eP9CRBwbEf9fdprrEeB7QEfWMzYT+K+U0q8GBkkp7QK+D/xuRHQAr6PSuzaUX6WUHq1a/jmVguxZVHq+NmWn0nYD/5qtHzTnQZwM/EXV4/+LSpHWWbXPvYM8N9m/Px+w7UhgOiO/z/dX3X+MSq+WpBJwcKU0AWRjgt4MtGVjgQCOplLInErl4P/ciDhykILqXmCo8TmPUSlO+j0H2Fm1nAbs/xEqvTEvTSndn4152kylGLkXeGZEdKSUdg/yXP8XOI/K59YPUkq9Q71eKuOLjqsqqJ4LbAMeonJqce4wjx+Y80D3Ap9JKQ1XzM0Etlc9967s/i4qxRhV2w4AD2RxF43w3JJKyJ4paWJYDhwEXkLl1NoC4MVUxvu8A/gRcB+wJiKOi4hjImJx9tgvAxdGxMKoeH7VgOvbgLdFRFtEnAH85gh5TKFSzOyOiGcCn+zfkFK6j8optC9lA9UnRcQrqx7bTWUM0h9SGUM1kk9HxFER8Qrgt4F/Sik9Cfwt8PmIeDZARHRGxLIa4vW7DFgdEXOzx0/NxpVVW5W9hplZvt/I1v8j8KGImB0Rk4H/A3wjK2CvAF4TEW+OiCMj4oTBBthLKh+LKWlieCfwlZTSL1JK9/ffqAx+PodKz9DrqQzI/gWV3qW3AKSU/onKYOevUxm31E1lgDZUCoXXA7uzON0j5PEFoJ1KD9FNVE6xVTuXyhiiHwMPAh/s35BS6gOuBmYD60d4nvuBX1HpCboCeG9K6cfZtouoDOC+KTvV+B0qvWU1SSldQ2UQ+7rs8duonHas9k1gE5Vi81+Av8vWX05lkP33gJ8BjwMrs7i/oDIW6iNUTh3eBpxaa16SmidSGqlHW5LKISI+AbwwpfT2YfZZAnwtpTRjqH1GU0Qk4AUppbub8fySxp5jpiSNC9lpwd+n0nslSaXhaT5JpRcR76EyQPv6lNL3mp2PJFXzNJ8kSVIO9kxJkiTlYDElSZKUQ9MGoE+bNi3NmjVr1J/n0Ucf5bjjjmt6DHMxl7GOYS7mYi7mYi7F2bRp00MppWcNurFZ17FZuHBhGgsbN24sRYyi4pjL6MUoKk5ZYhQVx1xGL0ZRccxl9GIUFcdcRi9GkXGGA9ySvDafJElS8SymJEmScrCYkiRJyqFUM6Dv37+fnTt38vjjjxcWc+rUqdx5551Nj9GsXI455hhmzJjBpEmTcj2vJEkaXKmKqZ07dzJlyhRmzZpFRBQSc+/evUyZMqXpMZqRS0qJhx9+mJ07dzJ79uxczytJkgZXqtN8jz/+OCeccEJhhdREFxGccMIJhfb0SZKkw5WqmAIspArm+ylJ0ugqXTHVTLt37+ZLX/pS3Y8788wz2b17d/EJSZKk0rOYqjJUMXXw4MFhH3fdddfR0dExSllJkqQyG9fFVPfmXhavuZHZF/8Li9fcSPfm3lzxLr74Yn7yk5+wYMECfu3Xfo2lS5fytre9jZe97GUALF++nIULFzJ37lz+5m/+5tDjZs2axUMPPcQ999zDi1/8Yt7znvcwd+5cXvva19LX15crJ0mSNLj+OmBr755C6oBGjdtiqntzL6vXb6V3dx8J6N3dx+r1W3O9kWvWrOF5z3set912G2vXruVHP/oRn/nMZ7j55psBuPzyy9m0aRO33HILl156KQ8//PDTYtx11128//3vZ/v27XR0dHD11Vc3nI8kSa2oiCKoug6AYuqARo3bYmrthh307T/89Fvf/oOs3bCjsOdYtGjRYVMKXHrppZx66qm87GUv49577+Wuu+562mNmz57NggULAFi4cCH33HNPYflIkjTeFVUEjUUdUKtxW0zt2j346bOh1jei+grUPT09fOc73+EHP/gBt99+O11dXYNOOXD00Ucfut/W1saBAwcKy0eSpPGuqCJoLOqAWo3bYuqkjva61tdiypQp7N27d9Bte/bs4fjjj+fYY4/lxz/+MTfddFPDzyNJ0kRVVBE0GnVAo8ZtMbVq2RzaJ7Udtq59Uhurls1pOOYJJ5zA4sWLOeWUU1i1atVh28444wwOHDjA/Pnz+fjHP35oULokSapdUUXQaNQBjSrV5WTqsbyrE6h0F+7a3cdJHe2sWjbn0PpGff3rXx90/dFHH831118/6Lb+cVHTpk1j27Zth9ZfeOGFuXKRJKnVrFo2h9Xrtx52qq+RIqi6DoC9dBZUBzRi3BZTUHkjm/GmSZKkxhRZBPXXAT09Paw8Z0mxidZhXBdTkiRp/ClLEVSUcTtmSpIkqQwspiRJUs3KMut4mXiaT5Ik1aR/ws2+/Qdh5lMTbgITegyzPVOSJKkmZZp1vEwspnKYPHkyALt27eLss88edJ8lS5Zwyy23DBvnC1/4Ao899tih5TPPPJPdu3cXlqckSUUo06zjZWIxVYCTTjqJq666quHHDyymrrvuOjo6OgrITJKk4pRp1vEyGd/F1JYr4fOnwKc6Kv9uuTJXuIsuuogvfelLh5Y/9alP8elPf5rXv/71nHbaacybN49vfvObT3vcPffcwymnnAJAX18fK1asYP78+bzlLW+hr++pav1DH/oQp59+OnPnzuWTn/wkULl48q5du1i6dClLly4FYNasWTz00EMAfO5zn+OUU07hlFNO4Qtf+MKh53vxi1/Me97zHubOnctrX/vaw55HkqSBihg4XqZZx8tk/BZTW66Ef/4A7LkXSJV///kDuQqqFStW8I1vfOPQ8pVXXsnv/d7vccUVV3DrrbeyceNGPvKRj5BSGjLGX//1X3PssceyZcsWPvrRj7Jp06ZD2z7+8Y9zyy23sGXLFr773e+yZcsWPvCBD3DSSSexceNGNm7ceFisTZs28ZWvfIUf/vCH3HTTTfzt3/4tt99+OwB33XUX73//+9m+fTsdHR1cffXVDb9uSVJr6x843pudjusfOF5vQbW8q5NLzppHZ9YT1dnRziVnzZvQg89hPBdTN/wR7B/QG7O/r7K+QV1dXTz44IPs2rWL22+/neOPP54TTzyRT3/608yfP5/XvOY19Pb28sADDwwZ43vf+x5vf/vbAZg/fz7z588/tO2aa67htNNOo6uri+3bt3PHHXcMm89//Md/8MY3vpHjjjuOyZMnc9ZZZ/Gf//mfAMyePZsFCxYAsHDhwkOXtJEkaaAiB44v7+rk+xe/inmdU/n+xa+a8IUUjOepEfbsrG99jc4++2yuuuoq7r//flasWMEVV1zBww8/zKZNm5g0aRKzZs3i8ccfHzZGRDxt3c9+9jMuvfRSNm3axPHHH8+73vWuEeMM1wN29NFHH7rf1tbmaT5J0pAcOD66xm/P1NQZ9a2v0YoVK1i3bh1XXXUVZ599Nnv27GHatGlMmjSJjRs38vOf/3zYx7/yla/kiiuuAGDbtm1s2bIFgEceeYTjjjuOqVOn8sADDxx20eQpU6awd+/eQWN1d3fz2GOP8eijj3LNNdfw8pe/PNfrkyRNPA4cH13jt5h69Sdg0oBGMKm9sj6HuXPnsnfvXjo7OznxxBM555xz2Lx5M6effjpXXHEFL3rRi4Z9/Pve9z727dvH/Pnz+dM//VMWLVoEwKmnnsr8+fOZO3cu7373u1m8ePGhx5x//vm87nWvOzQAvd9pp53Gu971LhYtWsRLX/pSzjvvPE499dRcr0+SNP7kHTzuwPHRVdNpvog4A/gLoA34ckppzYDtU4GvAc/NYv5ZSukrBed6uPlvrvx7wx9VTu1NnVEppPrX57B169ZD96dNm8YNN9zAlClTnrbfvn37gMqv77Zt2wZAe3s769atGzTuZZddNmiclStXsnLlykPL1eOfPvzhD/PhD3/40PLevXsPez6ACy+8sMZXJkkab4qYdbx/v8oYqb10drSzatkcxzsVZMRiKiLagC8CvwXsBG6OiG+llKpHT78fuCOl9PqIeBawIyKuSCk9MSpZ95v/5kKKJ0mSymq4weP1FEPLuzpZ3tVJT08PK89ZUnCWE1stp/kWAXenlH6aFUfrgDcM2CcBU6Iy8noy8F/AgUIzlSRpAnLwePnFcL8YA4iIs4EzUkrnZcvnAi9NKV1Qtc8U4FvAi4ApwFtSSv8ySKzzgfMBpk+fvnDg6bCpU6fy/Oc/P9cLGujgwYO0tbWNvOMox2hmLnfffTd79uw5bN2+ffsOXQ6nUUXEMJfRi2Eu5mIurZHLjvv38sTBJwGY3g4PZDXUUW1HMOc5Tx86Mpq5FB2nTLmMZOnSpZtSSqcPujGlNOwNeBOVcVL9y+cCfzlgn7OBzwMBPB/4GfCM4eIuXLgwDXTHHXekJ5988mnr83jkkUdKEaOoOPXGePLJJ9Mdd9zxtPUbN27MnUsRMYqK02q5tNrrKSqOuYxejKLimEvxMa65dWd60ceuTydfdG269Gvd6eSLrk0v+tj16Zpbd455LkXHKVMuIwFuSUPUNLWc5tsJzKxangHsGrDP7wHrs+e7Oyumhv/Z2yCOOeYYHn744WHnV1LtUko8/PDDHHPMMc1ORZImpCIu4eKs4+VXy6/5bgZeEBGzgV5gBfC2Afv8Ang18O8RMR2YA/y03mRmzJjBzp07+eUvf1nvQ4f0+OOP5y4miojRrFyOOeYYZszIN/eWJKl+RfwKr5+Dx8ttxGIqpXQgIi4ANlCZGuHylNL2iHhvtv0y4I+Br0bEViqn+i5KKT1UbzKTJk1i9uzZ9T5sWD09PXR1dTU9RtlykSSNrqJ+hafyq2meqZTSdcB1A9ZdVnV/F/DaYlOTJGn88ld4E8f4nQFdkqQS8xIuE4fFlCRJo8BLuEwcNZ3mkyRJ9fESLhOHxZQkSaPEX+FNDJ7mkyRpEEXMEaWJwZ4pSZIGKHKOKLU+e6YkSRpguDmipIEspiRJGsA5olQPiylJkgZwjijVw2JKkqQBnCNK9XAAuiRJAzhHlOphMSVJ0iCcI0q18jSfJKmlOD+Uxpo9U5KkluH8UGoGe6YkSS3D+aHUDBZTkqSW4fxQagaLKUlSy3B+KDWDxZQkqWU4P5SawQHokqSW4fxQagaLKUlSS3F+KI01T/NJkiTlYDElSZKUg8WUJKk0nL1c45HFlCQptyKKoP7Zy3uzOaH6Zy+3oFLZWUxJknIpqghy9nKNVxZTkqRciiqCnL1c45XFlCQpl6KKIGcv13hlMSVJyqWoIsjZyzVeWUxJknIpqgha3tXJJWfNozMrwjo72rnkrHnOXq7Ss5iSpAku7y/xiiyClnd18v2LX8W8zql8/+JXWUhpXPByMpI0gfX/Eq9v/0GY+dQv8YC6Chkv4aKJzJ4pSZrAnI5Ays9iSpImMKcjkPKzmJKkCczpCKT8LKYkaQJzOgIpPwegS9IE1j/IvDJGai+dHe2sWjbHX9FJdbCYkqQJzl/iSfl4mk+SJCkHiylJkqQcLKYkaZzKO3O5pGLUVExFxBkRsSMi7o6IiwfZvioibstu2yLiYEQ8s/h0JUnw1Mzlvdl8UP0zl1tQSWNvxGIqItqALwKvA14CvDUiXlK9T0ppbUppQUppAbAa+G5K6b9GIV9JEs5cLpVJLT1Ti4C7U0o/TSk9AawD3jDM/m8F/rGI5CSpVeU9RefM5VJ51FJMdQL3Vi3vzNY9TUQcC5wBXJ0/NUlqTUWconPmcqk8IqU0/A4RbwKWpZTOy5bPBRallFYOsu9bgLenlF4/RKzzgfMBpk+fvnDdunU50x/Zvn37mDx5ctNjmIu5jHUMcylvLjvu38sTB58EYHo7PJB1Jh3VdgRznjOlphi7+/bT+6s+nkzpUIwjIug8vp2O9kkN5dXs98VczKXZcYazdOnSTSml0wfdmFIa9gb8OrChank1sHqIfa8B3jZSzJQSCxcuTGNh48aNpYhRVBxzGb0YRcUpS4yi4phL8TFmXXRtOjm7Xfq17kP3Z110bV1xrrl1Z3r5JTekS7/WnV5+yQ3pmlt3NpxTSs1/X4qOYy6jF6OoOGXKZSTALWmImqaWGdBvBl4QEbOBXmAF8LaBO0XEVOA3gbfXW+1J0kRyUkf7oVN8A9fXw5nLpXIYccxUSukAcAGwAbgTuDKltD0i3hsR763a9Y3At1NKj45OqpLUGry4sNRaappnKqV0XUrphSml56WUPpOtuyyldFnVPl9NKa0YrUQlqQyKmChzeVcnl5w1j86sJ6qzo51LzprnxYWlccoLHUtSjfp/hde3/yDMfOpXeEDdhZCn6KTW4eVkJKlGTpQpaTAWU5JUIyfKlDQYiylJqpETZUoajMWUJNXIX+FJGowD0CWpRv2DzCtjpPbS2dHOqmVz/BWeNMFZTElSHfwVnqSBPM0nSZKUg8WUJElSDhZTkiRJOVhMSZIk5WAxJWnCKOK6epI0kL/mkzQhFHldPUmqZs+UpAnB6+pJGi0WU5ImBK+rJ2m0WExJmhC8rp6k0WIxJWlC8Lp6kkaLA9AlTQheV0/SaLGYkjRheF09SaPB03ySJEk5WExJkiTlYDElqfScuVxSmTlmSlKpOXO5pLKzZ0pSqTlzuaSys5iSVGrOXC6p7CymJI2qvOOdnLlcUtlZTEkaNf3jnXqzXqT+8U71FFTOXC6p7CymJI2aIsY7Le/q5JKz5tGZ9UR1drRzyVnzHHwuqTT8NZ+kUVPUeCdnLpdUZvZMSRo1jneSNBFYTEkaNY53kjQReJpP0qjpH9dUGSO1l86OdlYtm+N4J0ktxWJK0qhyvJOkVudpPkmSpBwspiRJknKwmJIkScrBYkqSJCkHiylJkqQcLKYkSZJysJiSJEnKwWJKkiQph5qKqYg4IyJ2RMTdEXHxEPssiYjbImJ7RHy32DQljbXuzb0sXnMjW3v3sHjNjXRv7m12SpJUSiPOgB4RbcAXgd8CdgI3R8S3Ukp3VO3TAXwJOCOl9IuIePYo5StpDHRv7mX1+q307T8IM6F3dx+r128F8FIwkjRALT1Ti4C7U0o/TSk9AawD3jBgn7cB61NKvwBIKT1YbJqSxtLaDTsqhVSVvv0Hs2vsSZKqRUpp+B0izqbS43Retnwu8NKU0gVV+3wBmATMBaYAf5FS+vtBYp0PnA8wffr0hevWrSvoZQxt3759TJ48uekxzMVcxjpGnjhbe/ccuj+9HR7oe2rbvM6pY5pL0THMxVzMxVwasXTp0k0ppdMH3ZhSGvYGvAn4ctXyucBfDtjnr4CbgOOAacBdwAuHi7tw4cI0FjZu3FiKGEXFMZfRi1FUnLLEyBPn5ZfckE6+6Np08kXXpku/1n3o/ssvuWHMcyk6RlFxzGX0YhQVx1xGL0ZRccqUy0iAW9IQNU0tp/l2AjOrlmcAuwbZ519TSo+mlB4CvgecWlOpJ6l0Vi2bQ/uktsPWtU9qY9WyOU3KSJLKq5Zi6mbgBRExOyKOAlYA3xqwzzeBV0TEkRFxLPBS4M5iU5U0VpZ3dXLJWfPo7GgHoLOjnUvOmufgc0kaxIi/5kspHYiIC4ANQBtweUppe0S8N9t+WUrpzoj4V2AL8CSV04LbRjNxSaNreVcny7s66enpYeU5S5qdjiSV1ojFFEBK6TrgugHrLhuwvBZYW1xqkiRJ5ecM6JIkSTlYTEmSJOVgMSVJkpSDxZQkSVIOFlOSJEk5WExJkiTlYDElSZKUg8WU1IK6N/eyeM2NbO3dw+I1N9K9ubfZKUlSy6pp0k5J40f35l5Wr99K3/6DMBN6d/exev1WAC8HI0mjwJ4pqcWs3bCjUkhV6dt/kLUbdjQpI0lqbRZTUovZtbuvrvWSpHwspqQWc1JHe13rJUn5WExJLWbVsjm0T2o7bF37pDZWLZvTpIwkqbU5AF1qMf2DzCtjpPbS2dHOqmVzHHwuSaPEYkpqQcu7Olne1UlPTw8rz1nS7HQkqaV5mk8qEeeHkqTxx54pqSScH0qSxid7pqSScH4oSRqfLKakknB+KEkanyympJJwfihJGp8spqSScH4oSRqfHIAulYTzQ0nS+GQxJZWI80NJ0vjjaT5JkqQcLKYkSZJysJiSJEnKwWJKkiQpB4spSZKkHCymJEmScrCYkiRJysFiSpIkKQeLKUmSpBwspqSCdG/uZfGaG9nau4fFa26ke3Nvs1OSJI0BLycjFaB7cy+r12+lb/9BmAm9u/tYvX4rgNfWk6QWZ8+UVIC1G3ZUCqkqffsPZhctliS1MospqQC7dvfVtV6S1DospqQCnNTRXtd6SVLrsJiSCrBq2RzaJ7Udtq59Uhurls1pUkaSpLHiAHSpAP2DzCtjpPbS2dHOqmVzHHwuSROAxZRUkOVdnSzv6qSnp4eV5yxpdjqSpDFS02m+iDgjInZExN0RcfEg25dExJ6IuC27faL4VCVJkspnxJ6piGgDvgj8FrATuDkivpVSumPArv+eUvrtUchRkiSptGrpmVoE3J1S+mlK6QlgHfCG0U1LkiRpfIiU0vA7RJwNnJFSOi9bPhd4aUrpgqp9lgBXU+m52gVcmFLaPkis84HzAaZPn75w3bp1xbyKYezbt4/Jkyc3PYa5mMtYxzAXczEXczGX4ixdunRTSun0QTemlIa9AW8Cvly1fC7wlwP2eQYwObt/JnDXSHEXLlyYxsLGjRtLEaOoOOYyejGKilOWGEXFMZfRi1FUHHMZvRhFxTGX0YtRZJzhALekIWqaWk7z7QRmVi3PoNL7VF2QPZJS2pfdvw6YFBHTai73JEmSxqlaiqmbgRdExOyIOApYAXyreoeIeE5ERHZ/URb34aKTlSRJKpsRf82XUjoQERcAG4A24PKU0vaIeG+2/TLgbOB9EXEA6ANWZF1ikiRJLa2mSTuzU3fXDVh3WdX9vwL+qtjUpLHTvbmXtRt2sGLmXj665kZnL5ck1cwZ0DXhdW/uZfX6rfTtPwgzoXd3H6vXbwWwoJIkjcgLHWvCW7thR6WQqtK3/2B2nT1JkoZnMaUJb9fuvrrWS5JUzWJKE95JHe11rZckqZrFlCa8Vcvm0D6p7bB17ZPaWLVsTpMykiSNJw5A14TXP8i8MkZqL50d7f6aT5JUM4spiUpBtbyrk56eHlaes6TZ6UiSxhFP80mSJOVgMSVJkpSDxZQkSVIOFlOSJEk5WExJkiTlYDGlca17cy+L19zI1t49LF5zI92be5udkiRpgnFqBI1bXqBYklQG9kxp3PICxZKkMrCY0rjlBYolSWVgMaVxywsUS5LKwGJK45YXKJYklYED0DVueYFiSVIZWExpXPMCxZKkZvM0nyRJUg4WU2oaJ9yUJLUCT/OpKZxwU5LUKuyZUlM44aYkqVVYTKkpnHBTktQqLKbUFE64KUlqFRZTagon3JQktQoHoKspnHBTktQqLKbUNE64KUlqBZ7mkyRJysFiSpIkKQeLKUmSpBwspiRJknKwmJIkScrBYkqSJCkHiylJkqQcLKYkSZJysJiSJEnKwWJKkiQpB4spSZKkHGoqpiLijIjYERF3R8TFw+z3axFxMCLOLi5FlU335l4Wr7mRrb17WLzmRro39zY7JUmSmmbEYioi2oAvAq8DXgK8NSJeMsR+nwU2FJ2kyqN7cy+r12+ld3cfAL27+1i9fqsFlSRpwqqlZ2oRcHdK6acppSeAdcAbBtlvJXA18GCB+alk1m7YQd/+g4et69t/kLUbdjQpI0mSmquWYqoTuLdqeWe27pCI6ATeCFxWXGoqo11Zj1St6yVJanWRUhp+h4g3ActSSudly+cCi1JKK6v2+Sfgz1NKN0XEV4FrU0pXDRLrfOB8gOnTpy9ct25dYS9kKPv27WPy5MlNj9Equey4fy9PHHwSgOnt8EBWQx3VdgRznjNlTHMpOkaZcmm112Mu5mIu5lL2XEaydOnSTSml0wfdmFIa9gb8OrChank1sHrAPj8D7slu+6ic6ls+XNyFCxemsbBx48ZSxCgqTrNzuebWnelFH7s+nXzRtenSr3Wnky+6Nr3oY9ena27dOea5FB2jqDhliVFUHHMZvRhFxTGX0YtRVBxzGb0YRcYZDnBLGqKmObKGYuxm4AURMRvoBVYAbxtQkM3uv1/VM9VdY7GncWR5V+UMb2WM1F46O9pZtWzOofWSJE00IxZTKaUDEXEBlV/ptQGXp5S2R8R7s+2Ok5pglnd1sryrk56eHlaes6TZ6UiS1FS19EyRUroOuG7AukGLqJTSu/KnJUmSND44A7okSVIOFlOSJEk5WExJkiTlYDElSZKUg8WUJElSDhZTkiRJOVhMSZIk5WAxJUmSlIPF1ATTvbmXxWtuZGvvHhavuZHuzb3NTkmSpHGtphnQ1Rq6N/eyev1W+vYfhJnQu7uP1eu3AnhtPUmSGmTP1ASydsOOSiFVpW//weyixZIkqREWUxPIrt19da2XJEkjs5iaQE7qaK9rvSRJGpnF1ASyatkc2ie1HbaufVIbq5bNaVJGkiSNfw5An0D6B5lXxkjtpbOjnVXL5jj4XJKkHCymJpjlXZ0s7+qkp6eHlecsaXY6kiSNe57mkyRJysFiSpIkKQeLKUmSpBwspiRJknKwmBonvKaeJEnl5K/5xgGvqSdJUnnZMzUOeE09SZLKy2JqHPCaesPYciV8/hS477bKv1uubHZGkqQJxmJqHGjZa+rlLYS2XAn//AHYc29lec+9lWULKknSGLKYGgda8pp6RRRCN/wR7B/QO7e/r7K+Gewlk6QJyWJqHFje1cklZ82jM+uJ6uxo55Kz5o3vwedFFEJ7dta3fjTZSyZJE5bF1DixvO37fP/oDzDviJ/x/aM/wPK27zc7pXyKKISmzqhv/WgqWy+ZVA97VaVcLKbGQO45olqx16OIQujVn4BJA8aNTWqvrB9rZeolAw+Oql0rfr5IY8xiapT1zxHVm/3yrn+OqLoKqlbs9SiiEJr/Znj9pTB1ZmV56szK8vw3F5dnrcrUS+bBUfVoxc8XaYxZTI2yQuaIKrLXoyw9FkUVQvPfDB/aBicuqPzbjEIKytVL5sFR9Shbr6o0DjkD+igrZI6oqTOe6mUYuL4e/T0W+/vgOTzVYwHNKULmv7ly6+mBt24b++cvUv/711+wTJ1ZKaSa8b56cFQ9ivp8kSYwe6ZGWSFzRBXV62GPxegqSy9ZmU45amhl6SUuU6+qNE5ZTI2yQuaIKuqUmD0W5VfEAbaog2NZDvatqEzj2so09lAapyymhlPAwaSwOaKK6PWwx6LcijrAFnFwLNPBvhWVrZe4LL2qGl1+QRo1FlNDKfBgUpo5ouzOL7ciD7B5D45F5uIH+NPZS6yxVrYvSC32uWAxNZSiDiZlasB255dbmQ6wReVSpvZfJvYSD63FDrKFyfu+lKk3tAU/FyymhlLUwaRMDRjszh9KGT7Ay3SALSqXsrX/IpRpXFurKeogW9T/5zJ8LvTnkfd9KdOXtRb8XLCYGkpRB5MyNWANrizfksp0gC0qlyJ7uMpwcCzTuLZWVMRBtsiCrAyfC1DM+1KmL2stOHeixdRQijqYlKkBa3Bl+ZZUpgNsUbkU0f7LdHAs07i2VlTEQbaov1GRQz3yHuyLeF/K9GWtqONiiQpei6mhFHUwKVMD1uDK1HtYpgNsEbkU0f7LdHAsU1uB0nwrL0wRB9mi/kZFxCnqYF/E+1KmL2stOHdiTcVURJwRETsi4u6IuHiQ7W+IiC0RcVtE3BIRv1F8qnUq4kOmiINJmRqwBmfv4egpov2X6eBYprZSom/lhSniIFvU36iIOEUd7IsqPsryZa0F504csZiKiDbgi8DrgJcAb42IlwzY7Qbg1JTSAuDdwJcLzrM+ZfuQKUsDLkqrfRu293B05W3/ZTo4lqmtlOhbeWGKOMgW9TcqIk5RB/tW/FLeYnMn1tIztQi4O6X005TSE8A64A3VO6SU9qWUUrZ4HJBoplb8kCmLshWqRWjFD6pWUqaDY5naSom+lRcq70G2yIuo541T5MG+1b6UF6FEX27iqRpoiB0izgbOSCmdly2fC7w0pXTBgP3eCFwCPBv4nymlHwwS63zgfIDp06cvXLduXSEv4mnuu+3Q3X1Hn8Tk/9711LYTF9Qdbt++fUyePDl3WkXEaXouD94BB5+oxKh+b9uOgmcP7LAc5VwKjlGmXFrt9eSO0/cr2Hsf+46cxuQDD8GUE6H9+ObFoQTvS9n+Lxb43ubOpcAYueL0/aryhTM9+dTfKI6oFGZNbHdNf1+KjFFwuxvO0qVLN6WUTh90Y0pp2BvwJuDLVcvnAn85zP6vBL4zUtyFCxemUfO5uSl98hkpffIZaePXP3/ofvrc3IbCbdy4sZC0iojT9Fw+OXXw9/aTU8c+l4JjFBWnLDGKimMuoxcjV5zbv5HSn0w//P/in0yvrJ/IuRQcI3ec27+R0ufmVt6Xz83N9Z7kzqXAGEXFKVMuIwFuSUPUNLWc5tsJzKxangHsGmJfUkrfA54XEdNqiD06Cur6697cy+I1N7K1dw+L19xI9+beApMcp0p0jlqa0Mp0ytGhFUPz9NyEUEsxdTPwgoiYHRFHASuAb1XvEBHPj4jI7p8GHAU8XHSyNSvgQ6Z7cy+r12+ld3flA6J3dx+r12+1oCrROWppwivLgbpVx29pcK32I6QCHDnSDimlAxFxAbABaAMuTyltj4j3ZtsvA34XeEdE7Af6gLdkXWLNM//NlVtPD7x1W90PX7thB337Dx62rm//QdZu2MHyrs6CkhyH+j+s+79xTp1ZKaT8tiVNXFNnPPWjlIHr1Vr6f4S0vw+ew1M/QoIJfRwYsZgCSCldB1w3YN1lVfc/C3y22NSaa9fuvrrWTyg5C1VJLebVn3jqANvPHuvWNNwp3QlcTDkD+hBO6miva70kTVhlGr+l0eUp3UFZTA1h1bI5tE9qO2xd+6Q2Vi2b06SMJKnEyjJ+S6PLHyENymJqCMu7OrnkrHl0Zj1RnR3tXHLWvIk9XkqSNLH5I6RB1TRmaqJa3tXJ8q5Oenp6WHnOkmanI0lSc/kjpEFZTEmSpNr5I6Sn8TSfJElSDhZTkiRJOVhMSZIk5WAxJUmSlIPFlCRJUg4WU5IkSTlYTEmSJOVgMSVJkpSDxZQkSVIOFlOSJEk5WExJkiTlYDElSZKUg8WUJElSDhZTkiRJOVhMSZIk5dCyxVT35l4Wr7mRrb17WLzmRro39zY7JUmS1IKObHYCo6F7cy+r12+lb/9BmAm9u/tYvX4rAMu7OpucnSRJaiUt2TO1dsOOSiFVpW//QdZu2NGkjCRJUqtqyWJq1+6+utZLkiQ1qiWLqZM62utaL0mS1KiWLKZWLZtD+6S2w9a1T2pj1bI5TcpIkiS1qpYcgN4/yLwyRmovnR3trFo2x8HnkiSpcC1ZTEGloFre1UlPTw8rz1nS7HQkSVKLasnTfJIkSWPFYkqSJCkHiylJkqQcLKYkSZJysJiSJEnKwWJKkiQpB4spSZKkHCymJEmScrCYkiRJysFiSpIkKYdIKTXniSN+Cfx8DJ5qGvBQCWKYi7mMdQxzMRdzMRdzKc7JKaVnDbolpdTSN+CWMsQwF3OZyK/HXMzFXMyl7LnkuXmaT5IkKQeLKUmSpBwmQjH1NyWJUVQccxm9GEXFKUuMouKYy+jFKCqOuYxejKLimMvoxSgyTkOaNgBdkiSpFUyEnilJkqRR07LFVERcHhEPRsS2HDFmRsTGiLgzIrZHxB82EOOYiPhRRNyexfh0jnzaImJzRFybI8Y9EbE1Im6LiFtyxOmIiKsi4sfZ+/PrdT5+TpZD/+2RiPhgA3l8KHtft0XEP0bEMfXGyOL8YRZjez15DNbOIuKZEfFvEXFX9u/xDcR4U5bLkxFxeo5c1mZ/oy0RcU1EdDQQ44+zx98WEd+OiJMayaVq24URkSJiWgO5fCoieqvazZmN5hIRKyNiR/Y+/2kDuXyjKo97IuK2RnKJiAURcVP//8mIWNRAjFMj4gfZ/+1/johnjBBj0M+2BtruUHFqbr/DxKi37Q4Vp+b2O1SMqu21tt2hcqm5/Q6XS51td6hcam6/w8Sot+0OFafm9htDHFPrbbuFa+ZPCUfzBrwSOA3YliPGicBp2f0pwP8DXlJnjAAmZ/cnAT8EXtZgPh8Gvg5cm+M13QNMK+D9/b/Aedn9o4COHLHagPupzOFRz+M6gZ8B7dnylcC7Gnj+U4BtwLHAkcB3gBc02s6APwUuzu5fDHy2gRgvBuYAPcDpOXJ5LXBkdv+zDebyjKr7HwAuaySXbP1MYAOVOeaGbYdD5PIp4MI6/76DxVma/Z2Pzpaf3cjrqdr+58AnGszl28DrsvtnAj0NxLgZ+M3s/ruBPx4hxqCfbQ203aHi1Nx+h4lRb9sdKk7N7XeoGA203aFyqbn9DhOj3rY74nFspPY7TC71tt2h4tTcfhnimFpv2y361rI9Uyml7wH/lTPGfSmlW7P7e4E7qRzA64mRUkr7ssVJ2a3ugWoRMQP4n8CX631s0bJvDa8E/g4gpfRESml3jpCvBn6SUmpkEtcjgfaIOJJKMbSrgRgvBm5KKT2WUjoAfBd4Yy0PHKKdvYFKsUn27/J6Y6SU7kwp7aglhxHifDt7TQA3ATMaiPFI1eJx1NB+h/n/93ngf+WMUZch4rwPWJNS+u9snwcbzSUiAngz8I8N5pKA/m/iUxmhDQ8RYw7wvez+vwG/O0KMoT7b6m27g8app/0OE6PetjtUnJrb7wif+fW03SKOHUPFqLftDptLLe13mBj1tt2h4tTcfoc5ptbVdovWssVU0SJiFtBFpQqu97FtWRfqg8C/pZTqjgF8gcp/5CcbeGy1BHw7IjZFxPkNxvgfwC+Br0TltOOXI+K4HDmtoIYD0UAppV7gz4BfAPcBe1JK327g+bcBr4yIEyLiWCrfsGY2EKff9JTSfVmO9wHPzhGrSO8Grm/kgRHxmYi4FzgH+ESDMX4H6E0p3d7I46tckJ22uTxHV/4LgVdExA8j4rsR8Ws58nkF8EBK6a4GH/9BYG32/v4ZsLqBGNuA38nuv4k62u+Az7aG226ez8gaYtTVdgfGaaT9VsfI03YHeU11t98BMRpuu0O8v3W13wExPkiDbXdAnLra7xDH1KZ+7lpM1SAiJgNXAx8c8C2nJimlgymlBVS+WS2KiFPqfP7fBh5MKW2q97kHsTildBrwOuD9EfHKBmIcSeU0w1+nlLqAR6l0q9YtIo6i8p/onxp47PFUvo3MBk4CjouIt9cbJ6V0J5XTCP8G/CtwO3Bg2AeNMxHxUSqv6YpGHp9S+mhKaWb2+AsaeP5jgY/SYCFW5a+B5wELqBTQf95gnCOB46mcHlgFXJl9Q2/EW2ngy0CV9wEfyt7fD5H1+Nbp3VT+P2+icvrkiVoelPezrcg4Q8Wot+0OFqfe9lsdI3vuhtruILnU3X4HidFQ2x3mb1Rz+x0kRkNtd5A4dbXfvMfUUVHEucKy3oBZ5BgzlZ46J7sB+HBBOX2S+sd8XALspDLe6X7gMeBrBeTyqXpzyR73HOCequVXAP/SYA5vAL7d4GPfBPxd1fI7gC8V8L78H+APGm1nwA7gxOz+icCOemNUre+hxjFTQ8UB3gn8ADi20RhV206u9f9UdRxgHpVvkfdktwNUehSfkyOXmv9/D/I3+ldgSdXyT4BnNfDeHgk8AMzI0V728NQ0NQE8kvNv9ELgRzXEeNpnW4Ntd8jPyFrb71AxGmi7w35e19J+B8bI0XZHymXE9jvE36iRtjvU+1tz+x0il0ba7kjvS03tt2r/TwIXNtJ2i7zZMzWMrNr/O+DOlNLnGozxrMh+hRIR7cBrgB/XEyOltDqlNCOlNIvKKbEbU0p198BExHERMaX/PpUBnnX/2jGldD9wb0TMyVa9Grij3jiZPN/qfwG8LCKOzf5Wr6ZyDr5uEfHs7N/nAmflyAngW1QOAmT/fjNHrFwi4gzgIuB3UkqPNRjjBVWLv0Od7RcgpbQ1pfTslNKsrB3vpDIQ9f46czmxavGNNNB+M93Aq7KYL6TyI4pGLpL6GuDHKaWdDeYBlXEmv5ndfxVQ9+nCqvZ7BPAx4LIR9h/qs62utlvQZ+SgMeptu8PEqbn9DhajkbY7TC41t99h3ttu6mi7I/yNamq/w8Soq+0O877U3H6HOaY293N3LCu3sbxRORjeB+yn0vh/v4EYv0FljNEW4LbsdmadMeYDm7MY26jhFz8jxFtCg7/mozLW6fbsth34aI48FgC3ZK+rGzi+gRjHAg8DU3Pk8Wkq/5G2Af9A9guXBuL8O5WC8Hbg1XnaGXACcAOVD5YbgGc2EOON2f3/pvLNcUODudwN3FvVfof9Jd4QMa7O3t8twD9TGdRbdy4Dtt/DyL+IGiyXfwC2Zrl8i+ybaANxjgK+lr2uW4FXNfJ6gK8C783ZXn4D2JS1vR8CCxuI8YdUfhn1/4A1ZL0Fw8QY9LOtgbY7VJya2+8wMeptu0PFqbn9DhWjgbY7VC41t99hYtTbdod8TdTYfofJpd62O1ScmtsvQxxTqbPtFn1zBnRJkqQcPM0nSZKUg8WUJElSDhZTkiRJOVhMSZIk5WAxJUmSlIPFlCRJUg4WU5IkSTlYTEmSJOXw/wOokJpSd93dCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_train, label = \"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Accuracy per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeca8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = axis_x, y = epoch_loss_train, label=\"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_loss_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c65a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
