{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0914f1f6",
   "metadata": {},
   "source": [
    "# Modelling:\n",
    "- This notebook uses the data obtained from Pre-Processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2674903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "tf.keras.utils.set_random_seed(442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 09:39:46.248657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:39:46.290888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:39:46.291010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# GPU Usage\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Set memory growth\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'Sadness': 0,\n",
    "    'Excited': 1,\n",
    "    'Happiness': 2,\n",
    "    'Anger' : 3,\n",
    "    'Frustration' : 4,\n",
    "    'Other' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf9f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    one_hot = np.zeros(6)\n",
    "    one_hot[labeldict[label]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(listOfLabels):\n",
    "    finalList = []\n",
    "    for label in listOfLabels:\n",
    "        finalList.append(one_hot_encode(label))\n",
    "    return np.array(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT_and_label(path):\n",
    "    emotion = re.match('.*/DATA/([a-zA-Z]+)/.*', path).groups()[0]\n",
    "    data, _ = librosa.load(path, sr=44100)\n",
    "    STFT = np.abs(librosa.stft(data))\n",
    "    return STFT, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(pathList): # Returns a list of x (batch_size, timesteps, feature), y (one_hot_encoded)\n",
    "    with mp.Pool() as p:\n",
    "        results = p.map(get_STFT_and_label, pathList)\n",
    "    # Preprocess x:\n",
    "    x = [item[0] for item in results]\n",
    "    # Flatten\n",
    "    x = [item for sublist in x for item in sublist]\n",
    "    # Zero-padding:\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", maxlen=1497, dtype = np.float32) # maxlen is after discovering the whole training data\n",
    "    # Reshaping so that the order is not messed up\n",
    "    x = x.reshape(-1, 1025, 1497)\n",
    "    # Transposing so that we have timesteps in dim 1\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    # Preprocess y:\n",
    "    y = [item[1] for item in results]\n",
    "    # one_hot_encode\n",
    "    y = one_hot_encode_list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5000",
   "metadata": {},
   "source": [
    "# Loading data: \n",
    "- We will load the data per predefined batch size, this is to reduce the memory used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fbf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_paths.pkl', 'rb') as f:\n",
    "    train_paths = pickle.load(f)\n",
    "with open('test_paths.pkl', 'rb') as f:\n",
    "    test_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc01af",
   "metadata": {},
   "source": [
    "### Verify the order is preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78ac068",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b=preprocess_input(train_paths[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69b089b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.8433094e-02, 1.3021269e-01, 1.4366210e-01, ..., 7.1329065e-04,\n",
       "        7.1328791e-04, 7.1328570e-04],\n",
       "       [2.4098808e-01, 3.4502932e-01, 3.7759700e-01, ..., 3.5598411e-04,\n",
       "        3.5598440e-04, 3.5597730e-04],\n",
       "       [1.0744733e-01, 4.3875551e-01, 4.3909615e-01, ..., 1.9026218e-10,\n",
       "        5.0048090e-09, 3.9142600e-09],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a12c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap, _ = get_STFT_and_label(train_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98dddd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.84330943e-02, 1.30212694e-01, 1.43662095e-01, ...,\n",
       "        7.13290647e-04, 7.13287911e-04, 7.13285699e-04],\n",
       "       [2.40988076e-01, 3.45029324e-01, 3.77597004e-01, ...,\n",
       "        3.55984113e-04, 3.55984404e-04, 3.55977303e-04],\n",
       "       [1.07447334e-01, 4.38755512e-01, 4.39096153e-01, ...,\n",
       "        1.90262181e-10, 5.00480901e-09, 3.91426003e-09],\n",
       "       ...,\n",
       "       [1.86033726e-01, 3.90104949e-01, 4.49180543e-01, ...,\n",
       "        1.15817738e-08, 7.79957610e-09, 2.30712938e-09],\n",
       "       [6.79136366e-02, 1.73671320e-01, 1.22675106e-01, ...,\n",
       "        4.54663714e-05, 4.54557012e-05, 4.54601577e-05],\n",
       "       [9.56224501e-02, 1.32804841e-01, 1.33970708e-01, ...,\n",
       "        1.33038717e-04, 1.33034511e-04, 1.33032721e-04]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "841dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batches of the pathList:\n",
    "def create_batches(pathList, batch_size):\n",
    "    ansList = [] # To store the final batched paths\n",
    "    tempList = [] # Temporary list\n",
    "    count = 0\n",
    "    while count < len(pathList):\n",
    "        tempList.append(pathList[count]) # Append the path\n",
    "        count += 1\n",
    "        if (count % batch_size) == 0: # if count is a multiple of batch_size\n",
    "            ansList.append(tempList)\n",
    "            tempList = []\n",
    "    if len(tempList) != 0: # If tempList is not empty\n",
    "        ansList.append(tempList) # Append the remaining values\n",
    "    return ansList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7d8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation datasets. The validation datasets are loaded entirely to the machine.\n",
    "x_val, y_val = preprocess_input(test_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f75e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1489, 1025)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28921b58",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920ba4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras API:\n",
    "# inp = layers.Input(shape=(None, 1025))\n",
    "# x = layers.Masking(mask_value=0.0)(inp)\n",
    "# total_seq, final_hidden_state, final_cell_state = layers.LSTM(256, return_state=True)(x)\n",
    "# x = layers.Dense(512, activation='relu')(final_hidden_state)\n",
    "# x = layers.Dense(256, activation='relu')(x)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8baf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 09:40:27.511188: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-22 09:40:27.513026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:40:27.513233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:40:27.513318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:40:27.876205: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:40:27.876324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:40:27.876390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-22 09:40:27.876710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6029 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Keras API: LSTM\n",
    "inp = layers.Input(shape=(None, 1025))\n",
    "x = layers.Masking(mask_value=0.0)(inp)\n",
    "lstm1, final_hidden_state1, final_cell_state1 = layers.LSTM(256, return_state=True, return_sequences=True)(x)\n",
    "lstm2, final_hidden_state2, final_cell_state2 = layers.LSTM(256, return_state=True, return_sequences=True)(lstm1, initial_state=[final_hidden_state1, final_cell_state1])\n",
    "lstm3, final_hidden_state3, final_cell_state3 = layers.LSTM(256, return_state=True)(lstm2, initial_state=[final_hidden_state2, final_cell_state2])\n",
    "concatenated = layers.Concatenate()([final_hidden_state3, final_cell_state3]) # concatenate the hidden state and cell state to make use of the information\n",
    "x = layers.Flatten()(concatenated) # Flattening the hidden state\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5661fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keras API: RNN + CNN\n",
    "# inp = layers.Input(shape=(None, 1025))\n",
    "\n",
    "# conv1 = layers.Conv1D(64, 1)(inp)\n",
    "# batchnorm1 = layers.BatchNormalization()(conv1)\n",
    "# maxpool1 = layers.MaxPool1D(5)(batchnorm1)\n",
    "\n",
    "# conv2 = layers.Conv1D(64, 3)(inp)\n",
    "# batchnorm2 = layers.BatchNormalization()(conv2)\n",
    "# maxpool2 = layers.MaxPool1D(5)(batchnorm2)\n",
    "\n",
    "# conv3 = layers.Conv1D(64, 5)(inp)\n",
    "# batchnorm3 = layers.BatchNormalization()(conv3)\n",
    "# maxpool3 = layers.MaxPool1D(5)(batchnorm3)\n",
    "\n",
    "# x = layers.Concatenate()([maxpool1, maxpool2, maxpool3])\n",
    "# lstm1, hidden1, cell1 = layers.LSTM(256, return_state=True, return_sequences=True)(x)\n",
    "# lstm2, hidden2, cell2 = layers.LSTM(256, return_state=True, return_sequences=True)(lstm1, initial_state=[hidden1, cell1])\n",
    "# lstm3, hidden3, cell3 = layers.LSTM(256, return_state=True)(lstm2, initial_state=[hidden2, cell2])\n",
    "\n",
    "# x = layers.Dense(256, activation='relu')(hidden3)\n",
    "# x = layers.Dense(128, activation='relu')(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "# x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "# model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3908fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 1025)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, None, 1025)   0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, None, 256),  1312768     ['masking[0][0]']                \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['lstm[0][0]',                   \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, 256),        525312      ['lstm_1[0][0]',                 \n",
      "                                 (None, 256),                     'lstm_1[0][1]',                 \n",
      "                                 (None, 256)]                     'lstm_1[0][2]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 512)          0           ['lstm_2[0][1]',                 \n",
      "                                                                  'lstm_2[0][2]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          65664       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6)            390         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,437,702\n",
      "Trainable params: 2,437,702\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b122fa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28347024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size is 32, epochs = 30\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer is Stochastic Gradient Descent\n",
    "# Loss function is Categorical Crossentropy\n",
    "optimizer = keras.optimizers.SGD()\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc25bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = create_batches(train_paths, batch_size=batch_size)\n",
    "validation_batch = create_batches(test_paths, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3369e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.CategoricalCrossentropy()\n",
    "validation_loss = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "538ba3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store epoch results:\n",
    "epoch_accuracy_train = []\n",
    "epoch_accuracy_val = []\n",
    "epoch_loss_train = []\n",
    "epoch_loss_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f3c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up, use graph execution\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training = True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # Update training accuracy\n",
    "    train_metrics.update_state(y, y_pred)\n",
    "    # Update training loss:\n",
    "    train_loss.update_state(y, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a850e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(x, y):\n",
    "    y_val_pred = model(x, training=False)\n",
    "    # Update metrics for validation\n",
    "    validation_metrics.update_state(y, y_val_pred)\n",
    "    validation_loss.update_state(y, y_val_pred)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1971ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 09:41:04.931520: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8400\n",
      "2022-05-22 09:41:05.287970: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 0: 1.7866\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7989\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7783\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7744\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7575\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7801\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7750\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7677\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7648\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7743\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7240\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7664\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7929\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7594\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7356\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2158\n",
      "Training loss over epoch: 1.7682\n",
      "Validation acc: 0.2650\n",
      "Validation loss: 1.7563\n",
      "Time taken: 240.96s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.7365\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8030\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7212\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7318\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7235\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7809\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7589\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7306\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7293\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7711\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6639\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7591\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7991\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7516\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7173\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2573\n",
      "Training loss over epoch: 1.7385\n",
      "Validation acc: 0.2675\n",
      "Validation loss: 1.7415\n",
      "Time taken: 229.33s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 1.7312\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8115\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7086\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7181\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7198\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7858\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7617\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7131\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7024\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7711\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6335\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7604\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7972\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7436\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7056\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2794\n",
      "Training loss over epoch: 1.7192\n",
      "Validation acc: 0.2675\n",
      "Validation loss: 1.7311\n",
      "Time taken: 225.68s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 1.7240\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8224\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6964\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7034\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7111\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7996\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7671\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7110\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6735\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7782\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6115\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7699\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7888\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7321\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6984\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2962\n",
      "Training loss over epoch: 1.7003\n",
      "Validation acc: 0.2708\n",
      "Validation loss: 1.7212\n",
      "Time taken: 227.02s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 1.7132\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8399\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6712\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6826\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6929\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8189\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7730\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7104\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6383\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7899\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5955\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7761\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7915\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7189\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6847\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3004\n",
      "Training loss over epoch: 1.6807\n",
      "Validation acc: 0.2650\n",
      "Validation loss: 1.7116\n",
      "Time taken: 227.48s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 1.6963\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8575\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6352\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6605\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6701\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8385\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7710\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7132\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6007\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.8005\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5907\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7652\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7967\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7033\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6550\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3106\n",
      "Training loss over epoch: 1.6604\n",
      "Validation acc: 0.2700\n",
      "Validation loss: 1.6997\n",
      "Time taken: 226.68s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 1.6612\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8642\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6074\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6430\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6390\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8493\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7485\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7124\n",
      "Seen so far: 2272 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 80: 1.5567\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.8006\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5860\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7518\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7977\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6898\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6165\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3192\n",
      "Training loss over epoch: 1.6390\n",
      "Validation acc: 0.2842\n",
      "Validation loss: 1.6838\n",
      "Time taken: 227.03s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 1.6074\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8642\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5824\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6414\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6048\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8454\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7190\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6931\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5246\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7952\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5891\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7428\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7860\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6645\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5809\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3227\n",
      "Training loss over epoch: 1.6171\n",
      "Validation acc: 0.3075\n",
      "Validation loss: 1.6667\n",
      "Time taken: 228.58s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 1.5397\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8481\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5561\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6186\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5672\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8407\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6803\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6567\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4949\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7687\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5624\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7239\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7475\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6444\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5333\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3348\n",
      "Training loss over epoch: 1.5947\n",
      "Validation acc: 0.3058\n",
      "Validation loss: 1.6694\n",
      "Time taken: 226.04s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 1.4977\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.8159\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5317\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5863\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5411\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7980\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6524\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6089\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4688\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7519\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5414\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6871\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6829\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6216\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5255\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3429\n",
      "Training loss over epoch: 1.5743\n",
      "Validation acc: 0.3050\n",
      "Validation loss: 1.6558\n",
      "Time taken: 227.29s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 1.4927\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7814\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5260\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5836\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5060\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7028\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6305\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5715\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4567\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7477\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5158\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6546\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5918\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5882\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5128\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3504\n",
      "Training loss over epoch: 1.5536\n",
      "Validation acc: 0.2983\n",
      "Validation loss: 1.6665\n",
      "Time taken: 228.01s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 1.5170\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7392\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5535\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5506\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4785\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6808\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5958\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5563\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4200\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7211\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5051\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6349\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5417\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6059\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5032\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3600\n",
      "Training loss over epoch: 1.5330\n",
      "Validation acc: 0.3192\n",
      "Validation loss: 1.6311\n",
      "Time taken: 228.18s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 1.4889\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7020\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5439\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4961\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4495\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6449\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5786\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5402\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4070\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7011\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5076\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6087\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5094\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5361\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4825\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3677\n",
      "Training loss over epoch: 1.5146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation acc: 0.3242\n",
      "Validation loss: 1.6468\n",
      "Time taken: 226.23s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 1.5028\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6535\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5170\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4726\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4411\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6238\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5391\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5107\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3717\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6814\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5033\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5813\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4414\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5064\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4961\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3800\n",
      "Training loss over epoch: 1.4976\n",
      "Validation acc: 0.3308\n",
      "Validation loss: 1.5997\n",
      "Time taken: 228.38s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 1.4605\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6435\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5346\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4374\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4099\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6177\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4976\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4836\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3445\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6605\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4572\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5642\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3906\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5083\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4762\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3885\n",
      "Training loss over epoch: 1.4858\n",
      "Validation acc: 0.3375\n",
      "Validation loss: 1.5940\n",
      "Time taken: 226.22s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 1.4510\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6547\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5529\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3342\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3916\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5890\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4912\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4734\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3530\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6508\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4560\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5292\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3883\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5017\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4493\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4035\n",
      "Training loss over epoch: 1.4700\n",
      "Validation acc: 0.3408\n",
      "Validation loss: 1.5904\n",
      "Time taken: 226.39s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 1.4410\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6983\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5330\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3593\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3829\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5664\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4646\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4581\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3558\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6183\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4261\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5356\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4224\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5090\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4321\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4002\n",
      "Training loss over epoch: 1.4659\n",
      "Validation acc: 0.3425\n",
      "Validation loss: 1.5864\n",
      "Time taken: 227.28s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 1.4168\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6823\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5530\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3320\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3664\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5567\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4490\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4594\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3040\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5904\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4237\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4936\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3894\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5034\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4276\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4094\n",
      "Training loss over epoch: 1.4433\n",
      "Validation acc: 0.3483\n",
      "Validation loss: 1.5822\n",
      "Time taken: 226.17s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 1.4168\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6686\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5368\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3396\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3482\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5489\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4668\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4840\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2756\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5818\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4224\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4342\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3222\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4369\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3928\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4200\n",
      "Training loss over epoch: 1.4322\n",
      "Validation acc: 0.3525\n",
      "Validation loss: 1.5793\n",
      "Time taken: 225.96s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 1.3862\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6882\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5239\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3495\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3660\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5071\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4074\n",
      "Seen so far: 1952 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 70: 1.4897\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3141\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5838\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3889\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4259\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3348\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3916\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4054\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4227\n",
      "Training loss over epoch: 1.4192\n",
      "Validation acc: 0.3517\n",
      "Validation loss: 1.5801\n",
      "Time taken: 223.82s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 1.3749\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6971\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5569\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3049\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3307\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4986\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4367\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4286\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2778\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6252\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3555\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3951\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3075\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3681\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3823\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4229\n",
      "Training loss over epoch: 1.4104\n",
      "Validation acc: 0.3533\n",
      "Validation loss: 1.5861\n",
      "Time taken: 226.36s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 1.3840\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6828\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5338\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3336\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3461\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4671\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4433\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4397\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2120\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5519\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3740\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3988\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3111\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3797\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3813\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4333\n",
      "Training loss over epoch: 1.3982\n",
      "Validation acc: 0.3633\n",
      "Validation loss: 1.5714\n",
      "Time taken: 225.82s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 1.3529\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6851\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5071\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3208\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3194\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4870\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3490\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6151\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2267\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6116\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3643\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4016\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3862\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4374\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3857\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4415\n",
      "Training loss over epoch: 1.3905\n",
      "Validation acc: 0.3683\n",
      "Validation loss: 1.5676\n",
      "Time taken: 225.21s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 1.3340\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7420\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4880\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3062\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3226\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4846\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4062\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4485\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2181\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5665\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3324\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3251\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3029\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2507\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3529\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4450\n",
      "Training loss over epoch: 1.3705\n",
      "Validation acc: 0.3592\n",
      "Validation loss: 1.5680\n",
      "Time taken: 224.40s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 1.3460\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6276\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4898\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3616\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2881\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4651\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3175\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4024\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2128\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5354\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2766\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3104\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2543\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4628\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3327\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4444\n",
      "Training loss over epoch: 1.3648\n",
      "Validation acc: 0.3592\n",
      "Validation loss: 1.5772\n",
      "Time taken: 223.52s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 1.2773\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7158\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4535\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3397\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2819\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4546\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2634\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.8114\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.1825\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4848\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2915\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3202\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2419\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2369\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3816\n",
      "Seen so far: 4512 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch: 0.4546\n",
      "Training loss over epoch: 1.3538\n",
      "Validation acc: 0.3567\n",
      "Validation loss: 1.5923\n",
      "Time taken: 223.49s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 1.3549\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6213\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4699\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3002\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3003\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4068\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2345\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7457\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.1662\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5054\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2289\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3118\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2531\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2247\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2947\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4650\n",
      "Training loss over epoch: 1.3401\n",
      "Validation acc: 0.3592\n",
      "Validation loss: 1.6058\n",
      "Time taken: 223.78s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 1.3635\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6620\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4885\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3309\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2688\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.3968\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2235\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3812\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2604\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6335\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2940\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2488\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1648\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2067\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3837\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4692\n",
      "Training loss over epoch: 1.3221\n",
      "Validation acc: 0.3692\n",
      "Validation loss: 1.5859\n",
      "Time taken: 223.64s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 1.3365\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.6043\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4316\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2636\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2590\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4052\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2087\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3851\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.1317\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5623\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2204\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2378\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1535\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3850\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2862\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4731\n",
      "Training loss over epoch: 1.3148\n",
      "Validation acc: 0.3675\n",
      "Validation loss: 1.6105\n",
      "Time taken: 223.21s\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 1.2968\n",
      "Seen so far: 32 samples\n",
      "Training loss (for one batch) at step 10: 1.7473\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3772\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2683\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2501\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4083\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.1790\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 2.3588\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2370\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5968\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2958\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2965\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1905\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2183\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3670\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4740\n",
      "Training loss over epoch: 1.3194\n",
      "Validation acc: 0.3542\n",
      "Validation loss: 1.6064\n",
      "Time taken: 224.93s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1))\n",
    "    for step, batch in enumerate(training_batch):\n",
    "        x, y = preprocess_input(batch)\n",
    "        \n",
    "        loss = train_step(x, y)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc)))\n",
    "    loss_train = train_loss.result()\n",
    "    print(\"Training loss over epoch: %.4f\" % (float(loss_train)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    # For validation data:\n",
    "    for val_batch in validation_batch:\n",
    "        x_val, y_val = preprocess_input(val_batch)\n",
    "        \n",
    "        valid_step(x_val, y_val)\n",
    "        \n",
    "\n",
    "    # Metrics\n",
    "    val_acc = validation_metrics.result()\n",
    "    loss_val = validation_loss.result()\n",
    "    validation_metrics.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    # Append to a list for graph:\n",
    "    epoch_accuracy_train.append(train_acc)\n",
    "    epoch_accuracy_val.append(val_acc)\n",
    "    epoch_loss_train.append(loss_train)\n",
    "    epoch_loss_val.append(loss_val)\n",
    "    \n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "    print(\"Validation loss: %.4f\" % (float(loss_val)))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd8c0e",
   "metadata": {},
   "source": [
    "# Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ace2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = [i+1 for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87577487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_train, label = \"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Accuracy per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeca8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = axis_x, y = epoch_loss_train, label=\"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_loss_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c65a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
