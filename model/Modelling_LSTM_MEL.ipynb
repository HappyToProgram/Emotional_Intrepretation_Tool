{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe86bf",
   "metadata": {},
   "source": [
    "# Modelling \n",
    "- This notebook uses the data obtained from Pre-Processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2674903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "tf.keras.utils.set_random_seed(442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:14:25.777693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:25.783313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:25.783414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# GPU Usage\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Set memory growth\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'Sadness': 0,\n",
    "    'Excited': 1,\n",
    "    'Happiness': 2,\n",
    "    'Anger' : 3,\n",
    "    'Frustration' : 4,\n",
    "    'Other' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf9f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    one_hot = np.zeros(6)\n",
    "    one_hot[labeldict[label]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(listOfLabels):\n",
    "    finalList = []\n",
    "    for label in listOfLabels:\n",
    "        finalList.append(one_hot_encode(label))\n",
    "    return np.array(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_and_label(path):\n",
    "    emotion = re.match('.*/DATA/([a-zA-Z]+)/.*', path).groups()[0]\n",
    "    data, _ = librosa.load(path, sr=44100)\n",
    "    mels = librosa.feature.melspectrogram(y=data, sr=44100, n_mels=256)\n",
    "    return mels, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(pathList): # Returns a list of x (batch_size, timesteps, feature), y (one_hot_encoded)\n",
    "    with mp.Pool() as p:\n",
    "        results = p.map(get_mel_and_label, pathList)\n",
    "    # Preprocess x:\n",
    "    x = [item[0] for item in results]\n",
    "    # Flatten\n",
    "    x = [item for sublist in x for item in sublist]\n",
    "    # Zero-padding:\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", maxlen=1497, dtype = np.float16) # maxlen is after discovering the whole training data\n",
    "    # Reshaping so that the order is not messed up\n",
    "    x = x.reshape(-1, 256, 1497)\n",
    "    # Transposing so that we have timesteps in dim 1\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    # Convert to tensor and of type tf.float16 for faster operation\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float16)\n",
    "    # Preprocess y:\n",
    "    y = [item[1] for item in results]\n",
    "    # one_hot_encode\n",
    "    y = one_hot_encode_list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5000",
   "metadata": {},
   "source": [
    "# Loading data: \n",
    "- We will load the data per predefined batch size, this is to reduce the memory used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fbf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_paths.pkl', 'rb') as f:\n",
    "    train_paths = pickle.load(f)\n",
    "with open('test_paths.pkl', 'rb') as f:\n",
    "    test_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batches of the pathList:\n",
    "def create_batches(pathList, batch_size):\n",
    "    ansList = [] # To store the final batched paths\n",
    "    tempList = [] # Temporary list\n",
    "    count = 0\n",
    "    while count < len(pathList):\n",
    "        tempList.append(pathList[count]) # Append the path\n",
    "        count += 1\n",
    "        if (count % batch_size) == 0: # if count is a multiple of batch_size\n",
    "            ansList.append(tempList)\n",
    "            tempList = []\n",
    "    if len(tempList) != 0: # If tempList is not empty\n",
    "        ansList.append(tempList) # Append the remaining values\n",
    "    return ansList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28921b58",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920ba4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:14:31.311746: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-30 13:14:31.312561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:31.312704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:31.312766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:31.611526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:31.611647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:31.611712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-30 13:14:31.611777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6108 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Keras API:\n",
    "inp = layers.Input(shape=(None, 256)) # Let's make to fixed so that we could use CNN more efficiently\n",
    "# LSTM:\n",
    "x_LSTM = layers.Masking(mask_value=0.0)(inp)\n",
    "total_seq1, hidden_state1, cell_state1 = layers.LSTM(64, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.3)(x_LSTM)\n",
    "total_seq2, hidden_state2, cell_state2 = layers.LSTM(64, dropout=0.3, return_state=True, recurrent_dropout=0.3)(total_seq1, initial_state=[hidden_state1, cell_state1])\n",
    "\n",
    "x = layers.concatenate([hidden_state2, cell_state2])\n",
    "\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3908fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 256)]  0           []                               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, None, 256)    0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, None, 64),   82176       ['masking[0][0]']                \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 64),         33024       ['lstm[0][0]',                   \n",
      "                                 (None, 64),                      'lstm[0][1]',                   \n",
      "                                 (None, 64)]                      'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['lstm_1[0][1]',                 \n",
      "                                                                  'lstm_1[0][2]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          16512       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6)            390         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 140,358\n",
      "Trainable params: 140,358\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b122fa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28347024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size is 32, epochs = 30\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer is Stochastic Gradient Descent\n",
    "# Loss function is Categorical Crossentropy\n",
    "optimizer = keras.optimizers.Adam() #amsgrad=True\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc25bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = create_batches(train_paths, batch_size=batch_size)\n",
    "validation_batch = create_batches(test_paths, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3369e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.CategoricalCrossentropy()\n",
    "validation_loss = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538ba3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store epoch results:\n",
    "epoch_accuracy_train = []\n",
    "epoch_accuracy_val = []\n",
    "epoch_loss_train = []\n",
    "epoch_loss_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up, use graph execution\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training = True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # Update training accuracy\n",
    "    train_metrics.update_state(y, y_pred)\n",
    "    # Update training loss:\n",
    "    train_loss.update_state(y, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a850e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(x, y):\n",
    "    y_val_pred = model(x, training=False)\n",
    "    # Update metrics for validation\n",
    "    validation_metrics.update_state(y, y_val_pred)\n",
    "    validation_loss.update_state(y, y_val_pred)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1971ed86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 13:14:46.137460: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 10: 1.7793\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7858\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7359\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.8403\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7183\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7697\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7380\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7775\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7159\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7597\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7726\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7784\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7271\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.8244\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2090\n",
      "Training loss over epoch: 1.7684\n",
      "Validation acc: 0.2200\n",
      "Validation loss: 1.7613\n",
      "Time taken: 794.13s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 10: 1.7298\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7411\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7257\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.8032\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7308\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7622\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6804\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5237\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7603\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7760\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6704\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7075\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6816\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7825\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2367\n",
      "Training loss over epoch: 1.7290\n",
      "Validation acc: 0.2625\n",
      "Validation loss: 1.6967\n",
      "Time taken: 785.78s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 10: 1.7493\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6638\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5135\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6933\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.8441\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.9474\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6841\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6155\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6851\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.8082\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7064\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7245\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.8348\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7364\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2504\n",
      "Training loss over epoch: 1.7133\n",
      "Validation acc: 0.2575\n",
      "Validation loss: 1.6960\n",
      "Time taken: 774.28s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 10: 1.6925\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7944\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6761\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6774\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6610\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6517\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7264\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7299\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6442\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6305\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.8127\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7678\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6158\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7633\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2598\n",
      "Training loss over epoch: 1.6994\n",
      "Validation acc: 0.2733\n",
      "Validation loss: 1.6831\n",
      "Time taken: 811.09s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 10: 1.7235\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6078\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6618\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7080\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6711\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7056\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6323\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.8182\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7655\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7660\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6662\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7991\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7409\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7072\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2590\n",
      "Training loss over epoch: 1.7005\n",
      "Validation acc: 0.2692\n",
      "Validation loss: 1.6744\n",
      "Time taken: 823.05s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 10: 1.6054\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6574\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6407\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7852\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6032\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6729\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7093\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7629\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6739\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6758\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6637\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7050\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6695\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7411\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2640\n",
      "Training loss over epoch: 1.6857\n",
      "Validation acc: 0.2867\n",
      "Validation loss: 1.6586\n",
      "Time taken: 789.59s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 10: 1.6575\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5586\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5167\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7299\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6211\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7034\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7930\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6888\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7437\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6016\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6024\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6406\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5860\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.7601\n",
      "Seen so far: 4512 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch: 0.2879\n",
      "Training loss over epoch: 1.6566\n",
      "Validation acc: 0.2792\n",
      "Validation loss: 1.6727\n",
      "Time taken: 782.85s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 10: 1.6781\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6041\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6131\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7181\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7795\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4606\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7052\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7759\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6687\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6597\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7432\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6988\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5272\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5732\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2973\n",
      "Training loss over epoch: 1.6408\n",
      "Validation acc: 0.3008\n",
      "Validation loss: 1.6409\n",
      "Time taken: 791.78s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 10: 1.5882\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7338\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7008\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7185\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5982\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6183\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5470\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5889\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5078\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6497\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4826\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4696\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5317\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6096\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3058\n",
      "Training loss over epoch: 1.6285\n",
      "Validation acc: 0.2942\n",
      "Validation loss: 1.6458\n",
      "Time taken: 832.83s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 10: 1.5962\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3141\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5531\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5118\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5881\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6195\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6196\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6240\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.8401\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5844\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6592\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6281\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6413\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4234\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3185\n",
      "Training loss over epoch: 1.6245\n",
      "Validation acc: 0.2783\n",
      "Validation loss: 1.6844\n",
      "Time taken: 800.61s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 10: 1.5059\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7002\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4984\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6170\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6112\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6483\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5681\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7073\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5270\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6943\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5117\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7444\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7859\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6742\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3204\n",
      "Training loss over epoch: 1.6130\n",
      "Validation acc: 0.2808\n",
      "Validation loss: 1.6668\n",
      "Time taken: 814.79s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 10: 1.6597\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3956\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6718\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5233\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6927\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5800\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4986\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5533\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6103\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5759\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4681\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5756\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6076\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6762\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3237\n",
      "Training loss over epoch: 1.5958\n",
      "Validation acc: 0.3142\n",
      "Validation loss: 1.6640\n",
      "Time taken: 821.38s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 10: 1.7210\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3181\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7190\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6697\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4789\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4341\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5325\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5999\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5211\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4051\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5434\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4973\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5777\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5088\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3340\n",
      "Training loss over epoch: 1.6013\n",
      "Validation acc: 0.3117\n",
      "Validation loss: 1.6601\n",
      "Time taken: 812.71s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 10: 1.4598\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7205\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5339\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4431\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6828\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4531\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4705\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7070\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3933\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4642\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6437\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5785\n",
      "Seen so far: 3872 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 130: 1.6734\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3718\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3406\n",
      "Training loss over epoch: 1.5698\n",
      "Validation acc: 0.3283\n",
      "Validation loss: 1.6550\n",
      "Time taken: 790.00s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 10: 1.7559\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3833\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6122\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5051\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5633\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5080\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4721\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5264\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6000\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6503\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.8462\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6852\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7465\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5834\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3581\n",
      "Training loss over epoch: 1.5681\n",
      "Validation acc: 0.3408\n",
      "Validation loss: 1.6370\n",
      "Time taken: 757.30s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 10: 1.7903\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5683\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6010\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4688\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5943\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3969\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3660\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4708\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5483\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4990\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.7345\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5675\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6327\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5669\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3594\n",
      "Training loss over epoch: 1.5510\n",
      "Validation acc: 0.3392\n",
      "Validation loss: 1.6420\n",
      "Time taken: 761.22s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 10: 1.4427\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4071\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7370\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4756\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6908\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5130\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5467\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5586\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4474\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7964\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6294\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6340\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5852\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3870\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3613\n",
      "Training loss over epoch: 1.5470\n",
      "Validation acc: 0.3192\n",
      "Validation loss: 1.6608\n",
      "Time taken: 801.88s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 10: 1.5385\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6319\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6135\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6749\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5079\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3985\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5559\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7423\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6136\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4633\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5371\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5409\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5373\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5269\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3738\n",
      "Training loss over epoch: 1.5306\n",
      "Validation acc: 0.3233\n",
      "Validation loss: 1.6503\n",
      "Time taken: 792.61s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 10: 1.6239\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4833\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6555\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4471\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5507\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5351\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5857\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4912\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4270\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5418\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4958\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4877\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4525\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5701\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3723\n",
      "Training loss over epoch: 1.5116\n",
      "Validation acc: 0.3225\n",
      "Validation loss: 1.6882\n",
      "Time taken: 789.64s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 10: 1.5443\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3131\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5012\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4775\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2976\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6871\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4206\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5293\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5484\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5975\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3600\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3413\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3724\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4467\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3823\n",
      "Training loss over epoch: 1.4976\n",
      "Validation acc: 0.3408\n",
      "Validation loss: 1.7132\n",
      "Time taken: 820.31s\n"
     ]
    }
   ],
   "source": [
    "# Custom Training loop:\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch))\n",
    "    # Shuffle the training batch for each epoch:\n",
    "    random.shuffle(training_batch)\n",
    "    for step, batch in enumerate(training_batch):\n",
    "        x, y = preprocess_input(batch)\n",
    "        \n",
    "        loss = train_step(x, y)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 10 == 0 and step != 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc)))\n",
    "    loss_train = train_loss.result()\n",
    "    print(\"Training loss over epoch: %.4f\" % (float(loss_train)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    # For validation data:\n",
    "    for val_batch in validation_batch:\n",
    "        x_val, y_val = preprocess_input(val_batch)\n",
    "        \n",
    "        valid_step(x_val, y_val)\n",
    "        \n",
    "\n",
    "    # Metrics\n",
    "    val_acc = validation_metrics.result()\n",
    "    loss_val = validation_loss.result()\n",
    "    validation_metrics.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    # Append to a list for graph:\n",
    "    epoch_accuracy_train.append(train_acc)\n",
    "    epoch_accuracy_val.append(val_acc)\n",
    "    epoch_loss_train.append(loss_train)\n",
    "    epoch_loss_val.append(loss_val)\n",
    "    \n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "    print(\"Validation loss: %.4f\" % (float(loss_val)))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "930217b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"MEL_LSTM.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd8c0e",
   "metadata": {},
   "source": [
    "# Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = [i+1 for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87577487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_train, label = \"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Accuracy per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeca8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = axis_x, y = epoch_loss_train, label=\"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_loss_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c65a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
