{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe86bf",
   "metadata": {},
   "source": [
    "# Modelling CONCAT\n",
    "- This notebook uses the data obtained from Pre-Processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2674903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import librosa\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed for Reproducibility\n",
    "tf.keras.utils.set_random_seed(442)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c25fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 20:00:48.350402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:48.359902: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:48.360082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# GPU Usage\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# Set memory growth\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44e0892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict = {\n",
    "    'Sadness': 0,\n",
    "    'Excited': 1,\n",
    "    'Happiness': 2,\n",
    "    'Anger' : 3,\n",
    "    'Frustration' : 4,\n",
    "    'Other' : 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf9f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label):\n",
    "    one_hot = np.zeros(6)\n",
    "    one_hot[labeldict[label]] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d30a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_list(listOfLabels):\n",
    "    finalList = []\n",
    "    for label in listOfLabels:\n",
    "        finalList.append(one_hot_encode(label))\n",
    "    return np.array(finalList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904a4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_and_label(path):\n",
    "    emotion = re.match('.*/DATA/([a-zA-Z]+)/.*', path).groups()[0]\n",
    "    data, _ = librosa.load(path, sr=44100)\n",
    "    mels = librosa.feature.melspectrogram(y=data, sr=44100, n_mels=256)\n",
    "    return mels, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "822e9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(pathList): # Returns a list of x (batch_size, timesteps, feature), y (one_hot_encoded)\n",
    "    with mp.Pool() as p:\n",
    "        results = p.map(get_mel_and_label, pathList)\n",
    "    # Preprocess x:\n",
    "    x = [item[0] for item in results]\n",
    "    # Flatten\n",
    "    x = [item for sublist in x for item in sublist]\n",
    "    # Zero-padding:\n",
    "    x = keras.preprocessing.sequence.pad_sequences(x, padding=\"post\", maxlen=1497, dtype = np.float16) # maxlen is after discovering the whole training data\n",
    "    # Reshaping so that the order is not messed up\n",
    "    x = x.reshape(-1, 256, 1497)\n",
    "    # Transposing so that we have timesteps in dim 1\n",
    "    x = x.transpose((0, 2, 1))\n",
    "    # Convert to tensor and of type tf.float16 for faster operation\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float16)\n",
    "    # Preprocess y:\n",
    "    y = [item[1] for item in results]\n",
    "    # one_hot_encode\n",
    "    y = one_hot_encode_list(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5000",
   "metadata": {},
   "source": [
    "# Loading data: \n",
    "- We will load the data per predefined batch size, this is to reduce the memory used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fbf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_paths.pkl', 'rb') as f:\n",
    "    train_paths = pickle.load(f)\n",
    "with open('test_paths.pkl', 'rb') as f:\n",
    "    test_paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "841dfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make batches of the pathList:\n",
    "def create_batches(pathList, batch_size):\n",
    "    ansList = [] # To store the final batched paths\n",
    "    tempList = [] # Temporary list\n",
    "    count = 0\n",
    "    while count < len(pathList):\n",
    "        tempList.append(pathList[count]) # Append the path\n",
    "        count += 1\n",
    "        if (count % batch_size) == 0: # if count is a multiple of batch_size\n",
    "            ansList.append(tempList)\n",
    "            tempList = []\n",
    "    if len(tempList) != 0: # If tempList is not empty\n",
    "        ansList.append(tempList) # Append the remaining values\n",
    "    return ansList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28921b58",
   "metadata": {},
   "source": [
    "# Modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920ba4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 20:00:51.339852: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-29 20:00:51.341070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:51.341266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:51.341400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:51.810384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:51.810563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:51.810704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-29 20:00:51.810816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6108 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Keras API:\n",
    "inp = layers.Input(shape=(1497, 256)) # Let's make to fixed so that we could use CNN more efficiently\n",
    "\n",
    "# CNN part:\n",
    "\n",
    "# first:\n",
    "x = layers.Conv1D(32, 5, padding='same', strides=3)(inp)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu and not relu (See Dying Relu problem that leads to overfitting)\n",
    "x = layers.Conv1D(32, 5, padding='same', strides=3)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu\n",
    "x = layers.MaxPool1D(2, padding='same')(x)\n",
    "\n",
    "# second:\n",
    "x = layers.Conv1D(32, 3, padding='same', strides=3)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu \n",
    "x = layers.Conv1D(32, 3, padding='same', strides=3)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu\n",
    "x = layers.MaxPool1D(2, padding='same')(x)\n",
    "\n",
    "# Third\n",
    "x = layers.Conv1D(32, 3, padding='same', strides=3)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu\n",
    "x = layers.Conv1D(32, 3, padding='same', strides=3)(x)\n",
    "x = layers.LeakyReLU()(x) # Activation is leaky relu\n",
    "x = layers.MaxPool1D(2, padding='same')(x)\n",
    "\n",
    "# Flattten:\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# LSTM:\n",
    "x_LSTM = layers.Masking(mask_value=0.0)(inp)\n",
    "total_seq1, final_hidden_state1, final_cell_state1 = layers.LSTM(128, return_state=True, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)(x_LSTM)\n",
    "total_seq2, final_hidden_state2, final_cell_state2 = layers.LSTM(128, return_state=True, dropout=0.3, recurrent_dropout=0.3)(total_seq1, initial_state=[final_hidden_state1, final_cell_state1])\n",
    "\n",
    "x = layers.concatenate([x, final_hidden_state2, final_cell_state2], axis = 1)\n",
    "\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3908fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1497, 256)]  0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 499, 32)      40992       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 499, 32)      0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 167, 32)      5152        ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 167, 32)      0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 84, 32)       0           ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 28, 32)       3104        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 28, 32)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 10, 32)       3104        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 10, 32)       0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 5, 32)       0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 2, 32)        3104        ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 2, 32)        0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 1, 32)        3104        ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 1, 32)        0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " masking (Masking)              (None, 1497, 256)    0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 1, 32)       0           ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 1497, 128),  197120      ['masking[0][0]']                \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 32)           0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 128),        131584      ['lstm[0][0]',                   \n",
      "                                 (None, 128),                     'lstm[0][1]',                   \n",
      "                                 (None, 128)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 288)          0           ['flatten[0][0]',                \n",
      "                                                                  'lstm_1[0][1]',                 \n",
      "                                                                  'lstm_1[0][2]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          36992       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 6)            390         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 432,902\n",
      "Trainable params: 432,902\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b122fa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28347024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size is 32, epochs = 30\n",
    "batch_size = 32\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f92c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer is Stochastic Gradient Descent\n",
    "# Loss function is Categorical Crossentropy\n",
    "optimizer = keras.optimizers.Adam() #amsgrad=True\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc25bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch = create_batches(train_paths, batch_size=batch_size)\n",
    "validation_batch = create_batches(test_paths, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3369e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics:\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "validation_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "train_loss = tf.keras.metrics.CategoricalCrossentropy()\n",
    "validation_loss = tf.keras.metrics.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "538ba3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list to store epoch results:\n",
    "epoch_accuracy_train = []\n",
    "epoch_accuracy_val = []\n",
    "epoch_loss_train = []\n",
    "epoch_loss_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f3c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up, use graph execution\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training = True)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # Update training accuracy\n",
    "    train_metrics.update_state(y, y_pred)\n",
    "    # Update training loss:\n",
    "    train_loss.update_state(y, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a850e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def valid_step(x, y):\n",
    "    y_val_pred = model(x, training=False)\n",
    "    # Update metrics for validation\n",
    "    validation_metrics.update_state(y, y_val_pred)\n",
    "    validation_loss.update_state(y, y_val_pred)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1971ed86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 20:01:00.972998: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8201\n",
      "2022-05-29 20:01:02.462025: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 10: 1.8139\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.9936\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7117\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.8037\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7058\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7324\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.7026\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 2.1866\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7215\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.8501\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5930\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7421\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6912\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.8273\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2390\n",
      "Training loss over epoch: 1.7717\n",
      "Validation acc: 0.2858\n",
      "Validation loss: 1.6815\n",
      "Time taken: 912.10s\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 10: 1.5882\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6364\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7183\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7223\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6063\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7016\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5455\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5061\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7761\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6593\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6777\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.7898\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7784\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6947\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2681\n",
      "Training loss over epoch: 1.7158\n",
      "Validation acc: 0.3067\n",
      "Validation loss: 1.6631\n",
      "Time taken: 972.10s\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 10: 1.6863\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.6388\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 3.8947\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6168\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6945\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7868\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5976\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7074\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 2.4441\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.7293\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6788\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6412\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7000\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5985\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2837\n",
      "Training loss over epoch: 1.6969\n",
      "Validation acc: 0.3125\n",
      "Validation loss: 1.6879\n",
      "Time taken: 916.97s\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 10: 1.7442\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.7273\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.8265\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5007\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6525\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6574\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6692\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6104\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5333\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6097\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6810\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6612\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6208\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6559\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.2942\n",
      "Training loss over epoch: 1.6464\n",
      "Validation acc: 0.3083\n",
      "Validation loss: 1.6992\n",
      "Time taken: 930.36s\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 10: 1.4326\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5508\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5958\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7439\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5642\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.7287\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5168\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.7473\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6094\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6297\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6017\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5973\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7157\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6898\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3158\n",
      "Training loss over epoch: 1.6440\n",
      "Validation acc: 0.3092\n",
      "Validation loss: 1.6806\n",
      "Time taken: 916.20s\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 10: 1.5651\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4553\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7404\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6713\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6572\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5442\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6952\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6532\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6967\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5629\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6268\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5991\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5758\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6456\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3173\n",
      "Training loss over epoch: 1.6188\n",
      "Validation acc: 0.2917\n",
      "Validation loss: 1.6903\n",
      "Time taken: 921.80s\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 10: 1.5802\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4536\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5993\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.7101\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6681\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5989\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6364\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4945\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6899\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5574\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.4547\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5858\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.6305\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5857\n",
      "Seen so far: 4512 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training acc over epoch: 0.3298\n",
      "Training loss over epoch: 1.6134\n",
      "Validation acc: 0.3017\n",
      "Validation loss: 1.6759\n",
      "Time taken: 910.24s\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 10: 1.5538\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5255\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3743\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6211\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.7181\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5239\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.6694\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6383\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5473\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4959\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6082\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6931\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4990\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3576\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3306\n",
      "Training loss over epoch: 1.5770\n",
      "Validation acc: 0.3242\n",
      "Validation loss: 1.7365\n",
      "Time taken: 890.66s\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 10: 1.4068\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4099\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7052\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.6111\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5557\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3835\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4243\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5309\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5257\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5744\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5736\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4400\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5686\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6525\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3500\n",
      "Training loss over epoch: 1.5606\n",
      "Validation acc: 0.3092\n",
      "Validation loss: 1.7429\n",
      "Time taken: 940.53s\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 10: 1.5499\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4166\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5405\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4701\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6241\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4920\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5210\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4025\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5336\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4999\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5628\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.6700\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4047\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5114\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3533\n",
      "Training loss over epoch: 1.5628\n",
      "Validation acc: 0.3225\n",
      "Validation loss: 1.6971\n",
      "Time taken: 942.43s\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 10: 1.4558\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5538\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4251\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5571\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6286\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4631\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.5086\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5758\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3654\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.6420\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 2.2735\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5665\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5268\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5309\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3554\n",
      "Training loss over epoch: 1.5492\n",
      "Validation acc: 0.3008\n",
      "Validation loss: 1.7161\n",
      "Time taken: 921.12s\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 10: 1.7033\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2346\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.6097\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2649\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5551\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6008\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4902\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4996\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5817\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5084\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3709\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4746\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5099\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5350\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3635\n",
      "Training loss over epoch: 1.5863\n",
      "Validation acc: 0.3133\n",
      "Validation loss: 1.7928\n",
      "Time taken: 950.99s\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 10: 1.5106\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 4.5932\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.5433\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.4731\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.3619\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4283\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4251\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4729\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4402\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3160\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6619\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4291\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4732\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5443\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3794\n",
      "Training loss over epoch: 1.5347\n",
      "Validation acc: 0.3075\n",
      "Validation loss: 1.7082\n",
      "Time taken: 952.34s\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 10: 1.3549\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3696\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4904\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5251\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.6231\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3680\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3980\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4407\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5981\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4054\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5897\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4783\n",
      "Seen so far: 3872 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 130: 1.5806\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.1798\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.3975\n",
      "Training loss over epoch: 1.4734\n",
      "Validation acc: 0.3192\n",
      "Validation loss: 1.7375\n",
      "Time taken: 934.93s\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 10: 1.5614\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2488\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3274\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3322\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4163\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2489\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4051\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5192\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.6831\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.4882\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.6148\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2800\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.7668\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4282\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4119\n",
      "Training loss over epoch: 1.4420\n",
      "Validation acc: 0.3075\n",
      "Validation loss: 1.8140\n",
      "Time taken: 946.07s\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 10: 1.3477\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.5497\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3061\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2300\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2160\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3400\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.1875\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3681\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4191\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5981\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5490\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2922\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5439\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4983\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4260\n",
      "Training loss over epoch: 1.4040\n",
      "Validation acc: 0.3358\n",
      "Validation loss: 2.0331\n",
      "Time taken: 959.22s\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 10: 1.1760\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2453\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4232\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.5026\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5780\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4385\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2709\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4099\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2083\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3548\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3669\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.5029\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5486\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4017\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4333\n",
      "Training loss over epoch: 1.4292\n",
      "Validation acc: 0.3117\n",
      "Validation loss: 1.8823\n",
      "Time taken: 969.63s\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 10: 1.3199\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3220\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.4616\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.3951\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4581\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4722\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3761\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.1724\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5673\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3217\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.0959\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2291\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3298\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.5578\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4523\n",
      "Training loss over epoch: 1.3511\n",
      "Validation acc: 0.3000\n",
      "Validation loss: 2.0489\n",
      "Time taken: 961.48s\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 10: 1.2696\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4115\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3871\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1912\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2370\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.6486\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.4809\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.5486\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2037\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.5603\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.3266\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2617\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.5435\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3489\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4519\n",
      "Training loss over epoch: 1.3599\n",
      "Validation acc: 0.3192\n",
      "Validation loss: 2.1233\n",
      "Time taken: 939.22s\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 10: 1.3500\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3141\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3890\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 2.0346\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2069\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3413\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.1787\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4001\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.7162\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1172\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1770\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3157\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4010\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.0818\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4606\n",
      "Training loss over epoch: 1.3286\n",
      "Validation acc: 0.3175\n",
      "Validation loss: 2.1466\n",
      "Time taken: 930.51s\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 10: 1.2012\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2380\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.1321\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2799\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.4856\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.5388\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3828\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4750\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.4608\n",
      "Seen so far: 2912 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 100: 11.3311\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5018\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.4612\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2639\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4111\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4675\n",
      "Training loss over epoch: 1.3771\n",
      "Validation acc: 0.3017\n",
      "Validation loss: 1.9950\n",
      "Time taken: 944.36s\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 10: 1.2810\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.4995\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.1259\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1808\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2379\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4126\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2878\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2475\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3566\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3862\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5691\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3004\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4769\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.6393\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4519\n",
      "Training loss over epoch: 1.3675\n",
      "Validation acc: 0.2950\n",
      "Validation loss: 1.9644\n",
      "Time taken: 908.43s\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 10: 1.5953\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3671\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.2839\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2839\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2372\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2545\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2153\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.6779\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2499\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.0564\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.0735\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.1699\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.3859\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2213\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4494\n",
      "Training loss over epoch: 1.3613\n",
      "Validation acc: 0.3075\n",
      "Validation loss: 2.0411\n",
      "Time taken: 966.83s\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 10: 1.1026\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.1372\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3885\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.0729\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.0353\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.4248\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2461\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.4105\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.2507\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3033\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2308\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2814\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4548\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2934\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.4760\n",
      "Training loss over epoch: 1.2819\n",
      "Validation acc: 0.3108\n",
      "Validation loss: 2.0688\n",
      "Time taken: 913.83s\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 10: 0.9412\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.1122\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.1767\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 0.9351\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2987\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.9758\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3479\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2303\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.5264\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2292\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1070\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.0541\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.4172\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4144\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5035\n",
      "Training loss over epoch: 1.2377\n",
      "Validation acc: 0.2833\n",
      "Validation loss: 2.2698\n",
      "Time taken: 939.95s\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 10: 1.1797\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.2781\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.1012\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1486\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2343\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.1012\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.2105\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 0.9126\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.1893\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.0922\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2636\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.0790\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.0539\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.1966\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5100\n",
      "Training loss over epoch: 1.1684\n",
      "Validation acc: 0.3158\n",
      "Validation loss: 2.4545\n",
      "Time taken: 988.08s\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 10: 0.9151\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 0.9812\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.3064\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2254\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2532\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.2620\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3235\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.0420\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.0812\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.2197\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.2606\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.0161\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2973\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.4397\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5148\n",
      "Training loss over epoch: 1.1732\n",
      "Validation acc: 0.3133\n",
      "Validation loss: 2.3374\n",
      "Time taken: 937.32s\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 10: 1.1848\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.3117\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 1.7761\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.0660\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2220\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.0069\n",
      "Seen so far: 1952 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss (for one batch) at step 70: 1.7129\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.3568\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9992\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1245\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.1932\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.2926\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9997\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.3676\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5304\n",
      "Training loss over epoch: 1.1726\n",
      "Validation acc: 0.3033\n",
      "Validation loss: 2.3445\n",
      "Time taken: 968.14s\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 10: 1.1208\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.1774\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.7765\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.2667\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.5201\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 1.3760\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.3497\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2991\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 1.3767\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.1739\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.5218\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.3549\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 1.2773\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2166\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5019\n",
      "Training loss over epoch: 1.2283\n",
      "Validation acc: 0.2850\n",
      "Validation loss: 2.3317\n",
      "Time taken: 951.93s\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 10: 1.0551\n",
      "Seen so far: 352 samples\n",
      "Training loss (for one batch) at step 20: 1.0269\n",
      "Seen so far: 672 samples\n",
      "Training loss (for one batch) at step 30: 0.9452\n",
      "Seen so far: 992 samples\n",
      "Training loss (for one batch) at step 40: 1.1812\n",
      "Seen so far: 1312 samples\n",
      "Training loss (for one batch) at step 50: 1.2325\n",
      "Seen so far: 1632 samples\n",
      "Training loss (for one batch) at step 60: 0.8308\n",
      "Seen so far: 1952 samples\n",
      "Training loss (for one batch) at step 70: 1.1563\n",
      "Seen so far: 2272 samples\n",
      "Training loss (for one batch) at step 80: 1.2009\n",
      "Seen so far: 2592 samples\n",
      "Training loss (for one batch) at step 90: 0.9216\n",
      "Seen so far: 2912 samples\n",
      "Training loss (for one batch) at step 100: 1.3365\n",
      "Seen so far: 3232 samples\n",
      "Training loss (for one batch) at step 110: 1.0205\n",
      "Seen so far: 3552 samples\n",
      "Training loss (for one batch) at step 120: 1.0979\n",
      "Seen so far: 3872 samples\n",
      "Training loss (for one batch) at step 130: 0.9960\n",
      "Seen so far: 4192 samples\n",
      "Training loss (for one batch) at step 140: 1.2884\n",
      "Seen so far: 4512 samples\n",
      "Training acc over epoch: 0.5350\n",
      "Training loss over epoch: 1.1334\n",
      "Validation acc: 0.3233\n",
      "Validation loss: 2.2317\n",
      "Time taken: 914.47s\n"
     ]
    }
   ],
   "source": [
    "# Custom Training loop:\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStart of epoch %d\" % (epoch))\n",
    "    # Shuffle the training batch for each epoch:\n",
    "    random.shuffle(training_batch)\n",
    "    for step, batch in enumerate(training_batch):\n",
    "        x, y = preprocess_input(batch)\n",
    "        \n",
    "        loss = train_step(x, y)\n",
    "        \n",
    "        # Log every 200 batches.\n",
    "        if step % 10 == 0 and step != 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_metrics.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc)))\n",
    "    loss_train = train_loss.result()\n",
    "    print(\"Training loss over epoch: %.4f\" % (float(loss_train)))\n",
    "    \n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_metrics.reset_states()\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    # For validation data:\n",
    "    for val_batch in validation_batch:\n",
    "        x_val, y_val = preprocess_input(val_batch)\n",
    "        \n",
    "        valid_step(x_val, y_val)\n",
    "        \n",
    "\n",
    "    # Metrics\n",
    "    val_acc = validation_metrics.result()\n",
    "    loss_val = validation_loss.result()\n",
    "    validation_metrics.reset_states()\n",
    "    validation_loss.reset_states()\n",
    "    \n",
    "    # Append to a list for graph:\n",
    "    epoch_accuracy_train.append(train_acc)\n",
    "    epoch_accuracy_val.append(val_acc)\n",
    "    epoch_loss_train.append(loss_train)\n",
    "    epoch_loss_val.append(loss_val)\n",
    "    \n",
    "    print(\"Validation acc: %.4f\" % (float(val_acc)))\n",
    "    print(\"Validation loss: %.4f\" % (float(loss_val)))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "930217b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"CNN_LSTM_CONCAT_MEL.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd8c0e",
   "metadata": {},
   "source": [
    "# Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = [i+1 for i in range(epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87577487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_train, label = \"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_accuracy_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Accuracy per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeca8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(x = axis_x, y = epoch_loss_train, label=\"train\")\n",
    "ax.scatter(x = axis_x, y = epoch_loss_val, label=\"validation\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set_title(\"Loss per epoch\")\n",
    "ax.set_xticks(ticks = axis_x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c65a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
